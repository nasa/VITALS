[
  {
    "objectID": "CHANGE_LOG.html",
    "href": "CHANGE_LOG.html",
    "title": "VITALS",
    "section": "",
    "text": "All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. _________________________________________________________________________\n\n\n\n\n\nImproved Finding Concurrent Data Notebook text/instructions\nRenamed contribute.md\nadded repo description\n\n\n\nRepository description\n\n\n\n\n\n\n\nUpdated notebook ROI to Carpenteria Salt Marsh\n\n\nAdded landcover.geojson\n\n\n\n\n\n\n\nUpdated contribute.md and added user contributed directory\n\n\nAdded user_contributed directory\n\n\n\n\n\n\n\nThis is the first update.\n\n\nFinding Concurrent Data Notebook"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "VITALS",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at LPDAAC@usgs.gov. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the [Zarr Developers][Github], available at [https://github.com/zarr-developers/.github/blob/main/CODE_OF_CONDUCT.md] and from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "VITALS",
    "section": "",
    "text": "Please submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.\n\nWe want your help! Even if you’re not a coder! There are several ways you can contribute to this repository:\n\nReport an Issue or make a recommendation\nUpdate code, documentation, notebooks or other files (even fixing typos)\nPropose a new notebook\n\nIn the sections below we outline how to approach each of these types of contributions. If you’re new to GitHub, you can sign up here. There are a bunch of great resources on the GitHub Quickstart page. The GitHub Cheatsheet is also quite helpful, even for experienced users. Please reach out to lpdaac@usgs.gov with questions or concerns.\n\n\nIf you’ve found a problem with the repository, we want to know about it! Please submit an Issue. Before submitting, we would appreciate if you check to see if a similar issue already exists. If not, create a new issue, providing as much detail as possible. Things like screenshots and code excerpts demonstrating the problem are very helpful!\n\n\n\nTo contribute a solution to an issue or make a change to files within the repository we’ve created a typical outline of how to do that below. If you want to make a simple change, like correcting a typo within a markdown document or other documentation, there’s a great video explaining how to do that without leaving the GitHub website here. To make a more complex change to a notebook, code, or other file follow the instructions below.\n\nPlease create an Issue or comment on an existing issue describing the changes you intend to make.\n\nCreate a fork of this repository. This will create your own copy of the repository. When working from your fork, you can do whatever you want, you won’t mess up anyone else’s work so you’re safe to try things out. Worst case scenario you can delete your fork and recreate it.\n\nClone your fork to your local computer or cloud workspace using your preferred command line interface after navigating to the directory you want to place the repository in:\ngit clone your-fork-repository-url\n\nChange directories to the one you cloned\n\ncd repository-name\n\nAdd the upstream repository, this is the original repository that you want to contribute to.\n\ngit remote add upstream original-repository-url\n\nYou can use the following to view the remote repositories:\n\ngit remote -v\n\nupstream, which refers to the original repository\n\norigin, which refers to your personal fork\n\nDevelop your contribution:\n\nCreate a new branch named appropriately for the feature you want to work on:\n\ngit checkout -b new-branch-name\n\nOften, updates to an upstream repository will occur while you are developing changes on your personal fork. You can pull the latest changes from upstream\n\ngit pull upstream dev\n\nYou can check the status of your local copy of the repository to see what changes have been made using:\n\ngit status\n\nCommit locally as you progress using git add and git commit.` For example, updating a readme.md file:\n\ngit add readme.md\ngit commit -m \"updated readme file\"\n\nYou can check the status of your local copy of the repository again to see what pending changes have not been added or committed using:\n\ngit status\n\nAfter making some changes, push your changes back to your fork on GitHub:\n\ngit push origin branch-name\n\nEnter username and password, depending on your settings, you may need to use a Personal access token\n\nTo submit your contribution, navigate to your forked repository GitHub page and make a pull request using the Compare &pull request green button. Make sure to select the base repository and its dev branch. Also select your forked repository as head repository and make sure compare shows your branch name. You can add your comments and press Create pull request green button. Our team will be notified and will review your suggested revisions.\n\nPlease submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.\n\n\n\n\n\nIn the spirit of open science, we want to minimize barriers to sharing code and examples. We have added a user_contributed directory to the repository for anyone to share examples of their work in notebook or code form. Documentation and descriptions do not need to be as thorough as the examples we’ve created, but we ask that you provide as much as possible. Follow the instructions above, placing your new notebook or module in a suitably named directory within the user_contributed directory. Be sure to remove any large datasets and indicate where users can retrieve them.\n\n\n\nThese contributing guidelines are adapted from the NASA Transform to Open Science GitHub, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT ecological problems for Scientific Insight",
    "section": "",
    "text": "The International Space Station is a critical asset for the Earth science community – both for advancing critical science and applications priorities, and as a platform for technology demonstrations/pathfinders. These benefits have been particularly significant in recent years, with the installation and operation of instruments such as ECOSTRESS, a multispectral thermal instrument, and EMIT, a visible to short wave infrared imaging spectrometer with best-in-class signal to noise - both acquiring data at field-scale (<70-m). With both sensors mounted on the ISS, there is an unprecedented opportunity to demonstrate the compounded benefits of working with both datasets. In this workshop we highlight the power of these tools when used together, through the use of open source tools and services, cloud compute resources to effectively combine data from ECOSTRESS and EMIT to perform scientific analyses and apply data to real world issues.\nThis workshop is hosted by NASA Land Processes Distributed Activate Archive Center(LP DAAC) and NASA Jet Propulsion Laboratory (JPL) with support from the NASA Openscapes project.\nHands-on exercises will be executed from a Jupyter Hub on the Openscapes 2i2c cloud instance."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT ecological problems for Scientific Insight",
    "section": "Learning Outcomes:",
    "text": "Learning Outcomes:\n\nImaging Spectroscopy and thermal measurements 101, the electromagnetic spectrum and sensor specific considerations\n\nHow to access EMIT and ECOSTRESS data\n\nData Preprocessing and Exploratory Analysis\n\nHow to manipulate, combine, and visualize EMIT and ECOSTRESS data"
  },
  {
    "objectID": "index.html#learning-focus",
    "href": "index.html#learning-focus",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT ecological problems for Scientific Insight",
    "section": "Learning Focus:",
    "text": "Learning Focus:\nPractical Skills for Science"
  },
  {
    "objectID": "index.html#knowledge-career-level",
    "href": "index.html#knowledge-career-level",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT ecological problems for Scientific Insight",
    "section": "Knowledge & Career Level:",
    "text": "Knowledge & Career Level:\nBeginner, Intermediate"
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT ecological problems for Scientific Insight",
    "section": "Target Audience:",
    "text": "Target Audience:\n\nEarth and Planetary Surface Processes\nHydrology\nGlobal Environmental Change\nOcean Sciences\nScience and Society\nBiogeosciences"
  },
  {
    "objectID": "index.html#contact-info",
    "href": "index.html#contact-info",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT ecological problems for Scientific Insight",
    "section": "Contact Info",
    "text": "Contact Info\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 12-05-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html",
    "href": "python/01_Finding_Concurrent_Data.html",
    "title": "VITALS",
    "section": "",
    "text": "Summary\nBoth the ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) and the Earth surface Mineral dust source InvesTigation (EMIT) instruments are located on the International Space Station (ISS). Their overlapping fields of view provide an unprecedented opportunity to demonstrate the compounded benefits of working with both datasets. In this notebook we will show how to utilize the earthaccess Python library to find concurrent ECOSTRESS and EMIT data.\n\n\n\nBackground\nThe ECOSTRESS instrument is a multispectral thermal imaging radiometer designed to answer three overarching science questions:\n\nHow is the terrestrial biosphere responding to changes in water availability?\nHow do changes in diurnal vegetation water stress the global carbon cycle?\nCan agricultural vulnerability be reduced through advanced monitoring of agricultural water consumptive use and improved drought estimation?\n\nThe ECOSTRESS mission is answering these questions by accurately measuring the temperature of plants. Plants regulate their temperature by releasing water through tiny pores on their leaves called stomata. If they have sufficient water they can maintain their temperature, but if there is insufficient water, their temperatures rise and this temperature rise can be measured with ECOSTRESS. The images acquired by ECOSTRESS are the most detailed temperature images of the surface ever acquired from space and can be used to measure the temperature of an individual farmers field.\nMore details about ECOSTRESS and its associated products can be found on the ECOSTRESS website and ECOSTRESS product pages hosted by the Land Processes Distributed Active Archive Center (LP DAAC).\nThe EMIT instrument is an imaging spectrometer that measures light in visible and infrared wavelengths. These measurements display unique spectral signatures that correspond to the composition on the Earth’s surface. The EMIT mission focuses specifically on mapping the composition of minerals to better understand the effects of mineral dust throughout the Earth system and human populations now and in the future. In addition, the EMIT instrument can be used in other applications, such as mapping of greenhouse gases, snow properties, and water resources.\nMore details about EMIT and its associated products can be found on the EMIT website and EMIT product pages hosted by the LP DAAC.\nRequirements\n- NASA Earthdata Account\n- No Python setup requirements if connected to the workshop cloud instance!\n- Set up Python Environment - See setup_instructions.md in the /setup/ folder\nLearning Objectives\n- How to use earthaccess to find concurrent EMIT and ECOSTRESS data.\n- How to export a list of files and download them programmatically.\nTutorial Outline\n\nSetup\n\nSearching for Data\n\nOrganizing and Filtering Results\nVisualizing Intersecting Coverage\nCreating a list of Asset URLs and Downloading\n\n\n\nImport the required Python libraries.\n# Import required libraries\nimport os\nimport folium\nimport earthaccess\nimport warnings\nimport folium.plugins\nimport pandas as pd\nimport geopandas as gpd\nimport math\n\nfrom branca.element import Figure\nfrom IPython.display import display\nfrom shapely import geometry\nfrom skimage import io\nfrom datetime import timedelta\nfrom shapely.geometry.polygon import orient\nfrom matplotlib import pyplot as plt\n\n\nTo download or stream NASA data you will need an Earthdata account, you can create one here. We will use the login function from the earthaccess library for authentication before downloading at the end of the notebook. This function can also be used to create a local .netrc file if it doesn’t exist or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them. This step is not necessary to conduct searches but is needed to download or stream data.\n\n\n\n\nBoth EMIT and ECOSTRESS products are hosted by the Land Processes Distributed Active Archive Center (LP DAAC). In this example we will use the cloud-hosted EMIT_L2A_RFL and ECOSTRESS_L2T_LSTE products available from the LP DAAC to find data. Any results we find for these products, should be available for other products within the EMIT and ECOSTRESS collections.\nTo find data we will use the earthaccess Python library. earthaccess searches NASA’s Common Metadata Repository (CMR), a metadata system that catalogs Earth Science data and associated metadata records. The results can then be used to download granules or generate lists granule search result URLs.\nUsing earthaccess we can search based on the attributes of a granule, which can be thought of as a spatiotemporal scene from an instrument containing multiple assets (ex: Reflectance, Reflectance Uncertainty, Masks for the EMIT L2A Reflectance Collection). We can search using attributes such as collection, acquisition time, and spatial footprint. This process can also be used with other EMIT or ECOSTRESS products, other collections, or different data providers, as well as across multiple catalogs with some modification.\n\n\nFor this example, our spatial region of interest (ROI) will be the a region near Santa Barbara, CA that contains the Jack and Laura Dangermond Preserve and the Sedgwick Reserve.\nIn this example, we will create a rectangular ROI surrounding these two reserves as well as some of the agricultural region between. Even though the shape is rectangular we elect to search using a polygon rather than a standard bounding box in earthaccess because bounding boxes will typically have a larger spatial extent, capturing a lot of area we may not be interested in. This becomes more important for searches with larger ROIs than our example here. To search for intersections with a polygon using earthaccess, we need to format our ROI as a counterclockwise list of coordinate pairs.\nOpen the geojson file containing the Dangermond and Sedgwick boundaries as a geodataframe, and check the coordinate reference system (CRS) of the data.\n\npolygon = gpd.read_file('../data/agu_workshop_roi.geojson')\npolygon.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nThe CRS is EPSG:4326 (WGS84), which is also the CRS we want the data in to submit for our search.\nNext, lets examine our polygon a bit closer.\n\npolygon\n\n\n\n\n\n  \n    \n      \n      Name\n      Acreage\n      geometry\n    \n  \n  \n    \n      0\n      Sedgwick Reserve\n      5874.210000\n      POLYGON ((-120.04041 34.74342, -120.04122 34.7...\n    \n    \n      1\n      Dangermond Preserve\n      24458.615397\n      POLYGON ((-120.47367 34.56987, -120.47427 34.5...\n    \n  \n\n\n\n\nWe can see this geodataframe consists of two polygons, that we want to include in our study site. We need to create an exterior boundary polygon containing these, and make sure the vertices are in counter-clockwise order to submit them in our query. To do this, create a polygon consisting of all the geometries, then create a bounding rectangle. This will give us a simple exterior polygon around our two ROIs. After that, use the orient function to place our coordinate pairs in counterclockwise order.\n# Merge all Polygon geometries and create external boundary\nroi_poly = polygon.unary_union.envelope\n# Re-order vertices to counterclockwise\nroi_poly = orient(roi_poly, sign=1.0)\nMake a GeoDataFrame consisting of the bounding box geometry.\n\ndf = pd.DataFrame({\"Name\":[\"ROI Bounding Box\"]})\nagu_bbox = gpd.GeoDataFrame({\"Name\":[\"ROI Bounding Box\"], \"geometry\":[roi_poly]},crs=\"EPSG:4326\")\nagu_bbox\n\n\n\n\n\n  \n    \n      \n      Name\n      geometry\n    \n  \n  \n    \n      0\n      ROI Bounding Box\n      POLYGON ((-120.49929 34.44230, -120.01175 34.4...\n    \n  \n\n\n\n\nWe can write this bounding box to a file for use in future notebooks.\n#agu_bbox.to_file('../data/roi_bbox.geojson', driver='GeoJSON')\nWe can go ahead and visualize our region of interest and the exterior boundary polygon containing ROIs. First add a function to help reformat bound box coordinates to work with leaflet notation.\n# Function to convert a bounding box for use in leaflet notation\n\ndef convert_bounds(bbox, invert_y=False):\n    \"\"\"\n    Helper method for changing bounding box representation to leaflet notation\n\n    ``(lon1, lat1, lon2, lat2) -> ((lat1, lon1), (lat2, lon2))``\n    \"\"\"\n    x1, y1, x2, y2 = bbox\n    if invert_y:\n        y1, y2 = y2, y1\n    return ((y1, x1), (y2, x2))\n\nfig = Figure(width=\"750px\", height=\"375px\")\nmap1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\nfig.add_child(map1)\n\n# Add Convex Hull Polygon\nfolium.GeoJson(agu_bbox,\n                name='bounding_box',\n                ).add_to(map1)\n\n# Add roi geodataframe\npolygon.explore(\n    \"Name\",\n    popup=True,\n    categorical=True,\n    cmap='Set3',\n    style_kwds=dict(opacity=0.7, fillOpacity=0.4),\n    name=\"Regions of Interest\",\n    m=map1\n)\n\nmap1.add_child(folium.LayerControl())\nmap1.fit_bounds(bounds=convert_bounds(polygon.unary_union.bounds))\ndisplay(fig)\n\n\n\n\n\nAbove we can see our regions of interest (ROIs) and the exterior boundary polygon containing the ROIs that we opened. We can hover over different areas to see name of each ROI.\nLastly, we need to convert our polygon to a list of coordinate pairs.\n# Set ROI as list of exterior polygon vertices as coordinate pairs\nroi = list(roi_poly.exterior.coords)\n\n\n\nWe need to specify which products we want to search for using their short-names. As mentioned above, we will conduct our search using the EMIT Level 2A Reflectance (EMITL2ARFL) and ECOSTRESS Level 2 Tiled Land Surface Temperature and Emissivity (ECO_L2T_LSTE).\n\nNote: Here we use the Tiled ECOSTRESS LSTE Product. This will also work with the gridded LSTE and the swath; however, the swath product does not have a browse image for the visualization in section 4 and will require additional processing for subsequent analysis.\n\n# Data Collections for our search\ncollections = ['EMITL2ARFL', 'ECO_L2T_LSTE']\n\n\n\nFor our date range, we’ll look at data collected in January to November of 2023. The date_range can be specified as a pair of dates, start and end (up to, not including).\n# Define Date Range\ndate_range = ('2023-01-01','2023-11-01')\n\n\n\nSubmit a query using earthaccess.\n\nresults = earthaccess.search_data(\n    short_name=collections,\n    polygon=roi,\n    temporal=date_range,\n    count=500\n)\n\nGranules found: 296\n\n\n\n\n\n\nAs we can see from above, the results object contains a list of objects with metadata and links. We can convert this to a more readable format, a dataframe. In addition, we can make it a geodataframe by taking the spatial metadata and creating a shapely polygon representing the spatial coverage, and further customize which information we want to use from other metadata fields.\nFirst, we define some functions to help us create a shapely object for our geodataframe, and retrieve the specific browse image URLs that we want. By default, the browse image selected by earthaccess is the first one in the list, but the ECO_L2_LSTE has several browse images, and we want to make sure we retrieve the png file, which is a preview of the LSTE.\n# Function to create shapely polygon of spatial coverage\ndef get_shapely_object(result:earthaccess.results.DataGranule):\n    # Get Geometry Keys\n    geo = result['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry']\n    keys = geo.keys()\n\n    if 'BoundingRectangles' in keys:\n        bounding_rectangle = geo['BoundingRectangles'][0]\n        # Create bbox tuple\n        bbox_coords = (bounding_rectangle['WestBoundingCoordinate'],bounding_rectangle['SouthBoundingCoordinate'],\n                    bounding_rectangle['EastBoundingCoordinate'],bounding_rectangle['NorthBoundingCoordinate'])\n        # Create shapely geometry from bbox\n        shape = geometry.box(*bbox_coords, ccw=True)\n    elif 'GPolygons' in keys:\n        points = geo['GPolygons'][0]['Boundary']['Points']\n        # Create shapely geometry from polygons\n        shape = geometry.Polygon([[p['Longitude'],p['Latitude']] for p in points])\n    else:\n         raise ValueError('Provided result does not contain bounding boxes/polygons or is incompatible.')\n    return(shape)\n\n# Retrieve png browse image if it exists or first jpg in list of urls\ndef get_png(result:earthaccess.results.DataGranule):\n    https_links = [link for link in result.dataviz_links() if 'https' in link]\n    if len(https_links) == 1:\n        browse = https_links[0]\n    elif len(https_links) == 0:\n        browse = 'no browse image'\n        warnings.warn(f\"There is no browse imagery for {result['umm']['GranuleUR']}.\")\n    else:\n        browse = [png for png in https_links if '.png' in png][0]\n    return(browse)\nNow that we have our functions we can create a dataframe, then calculate and add our shapely geometries to make a geodataframe. After that, add a column for our browse image urls and print the number of granules in our results, so we can monitor the quantity we are working with a we winnow down to the data we want.\n\n# Create Dataframe of Results Metadata\nresults_df = pd.json_normalize(results)\n# Create shapely polygons for result\ngeometries = [get_shapely_object(results[index]) for index in results_df.index.to_list()]\n# Convert to GeoDataframe\ngdf = gpd.GeoDataFrame(results_df, geometry=geometries, crs=\"EPSG:4326\")\n# Remove results df, no longer needed\ndel results_df\n# Add browse imagery links\ngdf['browse'] = [get_png(granule) for granule in results]\ngdf['shortname'] = [result['umm']['CollectionReference']['ShortName'] for result in results]\n# Preview GeoDataframe\nprint(f'{gdf.shape[0]} granules total')\n\n296 granules total\n\n\nPreview our geodataframe to get an idea what it looks like.\n\ngdf.head()\n\n\n\n\n\n  \n    \n      \n      size\n      meta.concept-type\n      meta.concept-id\n      meta.revision-id\n      meta.native-id\n      meta.provider-id\n      meta.format\n      meta.revision-date\n      umm.TemporalExtent.RangeDateTime.BeginningDateTime\n      umm.TemporalExtent.RangeDateTime.EndingDateTime\n      ...\n      umm.Platforms\n      umm.MetadataSpecification.URL\n      umm.MetadataSpecification.Name\n      umm.MetadataSpecification.Version\n      umm.SpatialExtent.HorizontalSpatialDomain.Geometry.GPolygons\n      umm.PGEVersionClass.PGEName\n      umm.CloudCover\n      geometry\n      browse\n      shortname\n    \n  \n  \n    \n      0\n      2.496260\n      granule\n      G2581836170-LPCLOUD\n      1\n      ECOv002_L2T_LSTE_25460_016_11SKU_20230101T1552...\n      LPCLOUD\n      application/echo10+xml\n      2023-01-07T13:31:07.584Z\n      2023-01-01T15:52:48.650Z\n      2023-01-01T15:53:40.620Z\n      ...\n      [{'ShortName': 'ISS', 'Instruments': [{'ShortN...\n      https://cdn.earthdata.nasa.gov/umm/granule/v1.6.5\n      UMM-G\n      1.6.5\n      NaN\n      NaN\n      NaN\n      POLYGON ((-119.06582 34.21003, -119.06582 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n    \n      1\n      0.292531\n      granule\n      G2586136519-LPCLOUD\n      1\n      ECOv002_L2T_LSTE_25486_005_10SGD_20230103T0743...\n      LPCLOUD\n      application/echo10+xml\n      2023-01-10T22:21:11.233Z\n      2023-01-03T07:43:30.510Z\n      2023-01-03T07:44:22.480Z\n      ...\n      [{'ShortName': 'ISS', 'Instruments': [{'ShortN...\n      https://cdn.earthdata.nasa.gov/umm/granule/v1.6.5\n      UMM-G\n      1.6.5\n      NaN\n      NaN\n      NaN\n      POLYGON ((-119.59844 34.20719, -119.59844 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n    \n      2\n      8.495430\n      granule\n      G2586136993-LPCLOUD\n      1\n      ECOv002_L2T_LSTE_25486_006_11SKU_20230103T0744...\n      LPCLOUD\n      application/echo10+xml\n      2023-01-10T22:21:57.631Z\n      2023-01-03T07:44:22.480Z\n      2023-01-03T07:45:14.450Z\n      ...\n      [{'ShortName': 'ISS', 'Instruments': [{'ShortN...\n      https://cdn.earthdata.nasa.gov/umm/granule/v1.6.5\n      UMM-G\n      1.6.5\n      NaN\n      NaN\n      NaN\n      POLYGON ((-119.06582 34.21003, -119.06582 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n    \n      3\n      5.103070\n      granule\n      G2586137000-LPCLOUD\n      1\n      ECOv002_L2T_LSTE_25486_006_10SGD_20230103T0744...\n      LPCLOUD\n      application/echo10+xml\n      2023-01-10T22:21:58.326Z\n      2023-01-03T07:44:22.480Z\n      2023-01-03T07:45:14.450Z\n      ...\n      [{'ShortName': 'ISS', 'Instruments': [{'ShortN...\n      https://cdn.earthdata.nasa.gov/umm/granule/v1.6.5\n      UMM-G\n      1.6.5\n      NaN\n      NaN\n      NaN\n      POLYGON ((-119.59844 34.20719, -119.59844 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n    \n      4\n      1.160680\n      granule\n      G2591892077-LPCLOUD\n      1\n      ECOv002_L2T_LSTE_25547_005_11SKU_20230107T0607...\n      LPCLOUD\n      application/echo10+xml\n      2023-01-18T19:13:09.017Z\n      2023-01-07T06:07:21.560Z\n      2023-01-07T06:08:13.530Z\n      ...\n      [{'ShortName': 'ISS', 'Instruments': [{'ShortN...\n      https://cdn.earthdata.nasa.gov/umm/granule/v1.6.5\n      UMM-G\n      1.6.5\n      NaN\n      NaN\n      NaN\n      POLYGON ((-119.06582 34.21003, -119.06582 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n  \n\n5 rows × 34 columns\n\n\n\nThere are a lot of columns with data that is not relevant to our goal, so we can drop those. To do that, list the names of columns.\n\n# List Column Names\ngdf.columns\n\nIndex(['size', 'meta.concept-type', 'meta.concept-id', 'meta.revision-id',\n       'meta.native-id', 'meta.provider-id', 'meta.format',\n       'meta.revision-date',\n       'umm.TemporalExtent.RangeDateTime.BeginningDateTime',\n       'umm.TemporalExtent.RangeDateTime.EndingDateTime',\n       'umm.OrbitCalculatedSpatialDomains', 'umm.GranuleUR',\n       'umm.AdditionalAttributes', 'umm.MeasuredParameters',\n       'umm.SpatialExtent.HorizontalSpatialDomain.Geometry.BoundingRectangles',\n       'umm.ProviderDates', 'umm.CollectionReference.ShortName',\n       'umm.CollectionReference.Version', 'umm.PGEVersionClass.PGEVersion',\n       'umm.RelatedUrls', 'umm.DataGranule.DayNightFlag',\n       'umm.DataGranule.Identifiers', 'umm.DataGranule.ProductionDateTime',\n       'umm.DataGranule.ArchiveAndDistributionInformation', 'umm.Platforms',\n       'umm.MetadataSpecification.URL', 'umm.MetadataSpecification.Name',\n       'umm.MetadataSpecification.Version',\n       'umm.SpatialExtent.HorizontalSpatialDomain.Geometry.GPolygons',\n       'umm.PGEVersionClass.PGEName', 'umm.CloudCover', 'geometry', 'browse',\n       'shortname'],\n      dtype='object')\n\n\nNow create a list of columns to keep and use it to filter the dataframe.\n\n# Create a list of columns to keep\nkeep_cols = ['meta.concept-id','meta.native-id', 'umm.TemporalExtent.RangeDateTime.BeginningDateTime','umm.TemporalExtent.RangeDateTime.EndingDateTime','umm.CloudCover','umm.DataGranule.DayNightFlag','geometry','browse', 'shortname']\n# Remove unneeded columns\ngdf = gdf[gdf.columns.intersection(keep_cols)]\ngdf.head()\n\n\n\n\n\n  \n    \n      \n      meta.concept-id\n      meta.native-id\n      umm.TemporalExtent.RangeDateTime.BeginningDateTime\n      umm.TemporalExtent.RangeDateTime.EndingDateTime\n      umm.DataGranule.DayNightFlag\n      umm.CloudCover\n      geometry\n      browse\n      shortname\n    \n  \n  \n    \n      0\n      G2581836170-LPCLOUD\n      ECOv002_L2T_LSTE_25460_016_11SKU_20230101T1552...\n      2023-01-01T15:52:48.650Z\n      2023-01-01T15:53:40.620Z\n      Day\n      NaN\n      POLYGON ((-119.06582 34.21003, -119.06582 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n    \n      1\n      G2586136519-LPCLOUD\n      ECOv002_L2T_LSTE_25486_005_10SGD_20230103T0743...\n      2023-01-03T07:43:30.510Z\n      2023-01-03T07:44:22.480Z\n      Night\n      NaN\n      POLYGON ((-119.59844 34.20719, -119.59844 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n    \n      2\n      G2586136993-LPCLOUD\n      ECOv002_L2T_LSTE_25486_006_11SKU_20230103T0744...\n      2023-01-03T07:44:22.480Z\n      2023-01-03T07:45:14.450Z\n      Night\n      NaN\n      POLYGON ((-119.06582 34.21003, -119.06582 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n    \n      3\n      G2586137000-LPCLOUD\n      ECOv002_L2T_LSTE_25486_006_10SGD_20230103T0744...\n      2023-01-03T07:44:22.480Z\n      2023-01-03T07:45:14.450Z\n      Night\n      NaN\n      POLYGON ((-119.59844 34.20719, -119.59844 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n    \n      4\n      G2591892077-LPCLOUD\n      ECOv002_L2T_LSTE_25547_005_11SKU_20230107T0607...\n      2023-01-07T06:07:21.560Z\n      2023-01-07T06:08:13.530Z\n      Night\n      NaN\n      POLYGON ((-119.06582 34.21003, -119.06582 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n  \n\n\n\n\nThis is looking better, but we can make it more readable by renaming our columns.\n\n# Rename some Columns\ngdf.rename(columns = {'meta.concept-id':'concept_id','meta.native-id':'granule',\n                       'umm.TemporalExtent.RangeDateTime.BeginningDateTime':'start_datetime',\n                      'umm.TemporalExtent.RangeDateTime.EndingDateTime':'end_datetime',\n                      'umm.CloudCover':'cloud_cover',\n                      'umm.DataGranule.DayNightFlag':'day_night'}, inplace=True)\ngdf.head()\n\n\n\n\n\n  \n    \n      \n      concept_id\n      granule\n      start_datetime\n      end_datetime\n      day_night\n      cloud_cover\n      geometry\n      browse\n      shortname\n    \n  \n  \n    \n      0\n      G2581836170-LPCLOUD\n      ECOv002_L2T_LSTE_25460_016_11SKU_20230101T1552...\n      2023-01-01T15:52:48.650Z\n      2023-01-01T15:53:40.620Z\n      Day\n      NaN\n      POLYGON ((-119.06582 34.21003, -119.06582 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n    \n      1\n      G2586136519-LPCLOUD\n      ECOv002_L2T_LSTE_25486_005_10SGD_20230103T0743...\n      2023-01-03T07:43:30.510Z\n      2023-01-03T07:44:22.480Z\n      Night\n      NaN\n      POLYGON ((-119.59844 34.20719, -119.59844 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n    \n      2\n      G2586136993-LPCLOUD\n      ECOv002_L2T_LSTE_25486_006_11SKU_20230103T0744...\n      2023-01-03T07:44:22.480Z\n      2023-01-03T07:45:14.450Z\n      Night\n      NaN\n      POLYGON ((-119.06582 34.21003, -119.06582 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n    \n      3\n      G2586137000-LPCLOUD\n      ECOv002_L2T_LSTE_25486_006_10SGD_20230103T0744...\n      2023-01-03T07:44:22.480Z\n      2023-01-03T07:45:14.450Z\n      Night\n      NaN\n      POLYGON ((-119.59844 34.20719, -119.59844 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n    \n      4\n      G2591892077-LPCLOUD\n      ECOv002_L2T_LSTE_25547_005_11SKU_20230107T0607...\n      2023-01-07T06:07:21.560Z\n      2023-01-07T06:08:13.530Z\n      Night\n      NaN\n      POLYGON ((-119.06582 34.21003, -119.06582 35.2...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      ECO_L2T_LSTE\n    \n  \n\n\n\n\n\nNote: If querying on-premises (not cloud) LP DAAC datasets, the meta.concept-id will not show as xxxxxx-LPCLOUD. For these datasets, the granule name can be retrieved from the umm.DataGranule.Identifiers column.\n\nWe can filter using the day/night flag as well, but this step will be unnecessary as we check to ensure all results from ECOSTRESS fall within an hour of resulting EMIT granules.\n# gdf = gdf[gdf['day_night'].str.contains('Day')]\nOur first step toward filtering the datasets will be to add a column with a datetime.\n\nYou may have noticed that the date format is similar for ECOSTRESS and EMIT, but the ECOSTRESS data has additionaly includes fractional seconds. If using the recommended lpdaac_vitals Windows environment, you will need to pass the format='ISO8601'argument to the to_datetime function, as shown in the commented-out line.\n\ngdf['datetime_obj'] = pd.to_datetime(gdf['start_datetime']) # 2i2c\n# gdf['datetime_obj'] = pd.to_datetime(gdf['start_datetime'], format='ISO8601') # Local ENV\nWe can roughly visualize the quantity of results by month at our location using a histogram with 8 bins (Jan up to Sept).\n\ngdf.hist(column='datetime_obj', by='shortname', bins=10, color='green', edgecolor='black', linewidth=1, sharey=True)\n\narray([<Axes: title={'center': 'ECO_L2T_LSTE'}>,\n       <Axes: title={'center': 'EMITL2ARFL'}>], dtype=object)\n\n\n\n\n\nNow we will separate the results into two dataframes, one for ECOTRESS and one for EMIT and print the number of results for each so we can monitor how many granules we’re filtering.\n\n# Suppress Setting with Copy Warning - not applicable in this use case\npd.options.mode.chained_assignment = None  # default='warn'\n\n# Split into two dataframes - ECO and EMIT\neco_gdf = gdf[gdf['granule'].str.contains('ECO')]\nemit_gdf = gdf[gdf['granule'].str.contains('EMIT')]\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')\n\n ECOSTRESS Granules: 261 \n EMIT Granules: 35\n\n\n\nemit_gdf.head()\n\n\n\n\n\n  \n    \n      \n      concept_id\n      granule\n      start_datetime\n      end_datetime\n      day_night\n      cloud_cover\n      geometry\n      browse\n      shortname\n      datetime_obj\n    \n  \n  \n    \n      23\n      G2623634492-LPCLOUD\n      EMIT_L2A_RFL_001_20230129T211308_2302914_003\n      2023-01-29T21:13:08Z\n      2023-01-29T21:13:20Z\n      Day\n      96.0\n      POLYGON ((-120.29004 35.49324, -121.08087 34.8...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      EMITL2ARFL\n      2023-01-29 21:13:08+00:00\n    \n    \n      37\n      G2631040813-LPCLOUD\n      EMIT_L2A_RFL_001_20230219T202939_2305013_002\n      2023-02-19T20:29:39Z\n      2023-02-19T20:29:51Z\n      Day\n      47.0\n      POLYGON ((-120.70791 35.54348, -121.19894 34.9...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      EMITL2ARFL\n      2023-02-19 20:29:39+00:00\n    \n    \n      40\n      G2631045418-LPCLOUD\n      EMIT_L2A_RFL_001_20230219T202951_2305013_003\n      2023-02-19T20:29:51Z\n      2023-02-19T20:30:14Z\n      Day\n      67.0\n      POLYGON ((-120.04838 35.03646, -120.54523 34.4...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      EMITL2ARFL\n      2023-02-19 20:29:51+00:00\n    \n    \n      45\n      G2631457332-LPCLOUD\n      EMIT_L2A_RFL_001_20230223T185536_2305412_003\n      2023-02-23T18:55:36Z\n      2023-02-23T18:55:48Z\n      Day\n      98.0\n      POLYGON ((-120.61175 35.58776, -121.10041 34.9...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      EMITL2ARFL\n      2023-02-23 18:55:36+00:00\n    \n    \n      46\n      G2631458885-LPCLOUD\n      EMIT_L2A_RFL_001_20230223T185548_2305412_004\n      2023-02-23T18:55:48Z\n      2023-02-23T18:56:00Z\n      Day\n      96.0\n      POLYGON ((-119.95147 35.08163, -120.44685 34.4...\n      https://data.lpdaac.earthdatacloud.nasa.gov/lp...\n      EMITL2ARFL\n      2023-02-23 18:55:48+00:00\n    \n  \n\n\n\n\nWe still haven’t filtered the locations where EMIT and ECOSTRESS have data at the same spatial location and timeframe. The EMIT acquisition mask has been added to ECOSTRESS, so in most cases if EMIT is collecting data, so will ECOSTRESS, but there are edge cases where this is not true. To do this we’ll use two filters to catch the edge-cases and provide an example that can be used with other datasets.\nFirst, since EMIT has a smaller swath width, we can can use a unary union of the spatial coverage present in our geodataframe to filter out ECOSTRESS granules that do not overlap with it.\n\n# Subset ECOSTRESS Granules in Geodataframe by intersection with EMIT granules\n## Create new column based on intersection with union of EMIT polygons.\neco_gdf['intersects'] = eco_gdf.intersects(emit_gdf.unary_union)\n## Apply subsetting\neco_gdf = eco_gdf[eco_gdf['intersects'] == True]\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')\n\n ECOSTRESS Granules: 261 \n EMIT Granules: 35\n\n\nIn this instance, our results aren’t narrowed because our region of interest is smaller than a single EMIT scene. If the spatial ROI was very large, this would be much more unlikely.\nAdditionally, we want to make sure that data in our results are collected at the same time. For EMIT and ECOSTRESS, the EMIT acquisition mask has been added to the ECOSTRESS mask, meaning that if there is an EMIT scene, there should also be an ECOSTRESS scene acquired at the same time. In practice, however, the timestamps on the scenes can vary slightly. In order to capture this slight variability, we need to use a range instead of a single timestamp to capture concurrent data. To do this, we’ll ensure all ECOSTRESS granule start times fall within 10 minutes of any of the EMIT granules in our results, and vice-versa.\nWrite a function to evaluate whether these datetime objects fall within 10 minutes of one another using the timedelta function.\n# Function to Filter timestamps that do not fall within a time_delta of timestamps from the other acquisition time\ndef concurrent_match(gdf_a:pd.DataFrame, gdf_b:pd.DataFrame, col_name:str, time_delta:timedelta):\n    \"\"\"\n    Cross references dataframes containing a datetime object column and keeps rows in \n    each that fall within the provided timedelta of the other. Acceptable time_delta examples:\n    \n    months=1\n    days=1\n    hours=1\n    minutes=1\n    seconds=1 \n\n    \"\"\"\n    # Match Timestamps from Dataframe A with Time-range of entries in Dataframe B\n    # Create empty list\n    a_list = []\n    # Iterate results for product a based on index values\n    for _n in gdf_b.index.to_list():\n        # Find where product b is within the window of each product a result\n        a_matches = (gdf_a[col_name] > gdf_b[col_name][_n]-time_delta) & (gdf_a[col_name] < gdf_b[col_name][_n]+time_delta)\n        # Append list with values\n        a_list.append(a_matches)\n    # Match Timestamps from Dataframe B with Time-range of entries in Dataframe A\n    # Create empty list\n    b_list =[]\n    for _m in gdf_a.index.to_list():\n        # Find where product a is within the window of each product b result\n        b_matches = (gdf_b[col_name] > gdf_a[col_name][_m]-time_delta) &  (gdf_b[col_name] < gdf_a[col_name][_m]+time_delta)\n        # Append list with values\n        b_list.append(b_matches)\n    # Filter Original Dataframes by summing list of bools, 0 = outside of all time-ranges\n    a_filtered = gdf_a.loc[sum(a_list) > 0]\n    b_filtered = gdf_b.loc[sum(b_list) > 0]\n    return(a_filtered, b_filtered)\nNow run our function.\n\n\neco_gdf, emit_gdf = concurrent_match(eco_gdf,emit_gdf, col_name='datetime_obj',time_delta=timedelta(minutes=10))\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')\n\n ECOSTRESS Granules: 29 \n EMIT Granules: 22\n\n\n\n\n\nNow that we have geodataframes containing some concurrent data, we can visualize them on a map using folium. It’s often difficult to visualize a large time-series of scenes, so we’ve included an example in Appendix A1 on how to filter to a single day.\n\n# Plot Using Folium\n\n# Create Figure and Select Background Tiles\nfig = Figure(width=\"750px\", height=\"375px\")\nmap1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\nfig.add_child(map1)\n\n# Plot STAC ECOSTRESS Results - note we must drop the datetime_obj columns for this to work\neco_gdf.drop(columns=['datetime_obj']).explore(\n    \"granule\",\n    categorical=True,\n    tooltip=[\n        \"granule\",\n        \"start_datetime\",\n        \"cloud_cover\",\n    ],\n    popup=True,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"ECOSTRESS\",\n    m=map1,\n    legend=False\n)\n\n# Plot STAC EMITL2ARFL Results - note we must drop the datetime_obj columns for this to work\nemit_gdf.drop(columns=['datetime_obj']).explore(\n    \"granule\",\n    categorical=True,\n    tooltip=[\n        \"granule\",\n        \"start_datetime\",\n        \"cloud_cover\",\n    ],\n    popup=True,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"EMIT\",\n    m=map1,\n    legend=False\n)\n\n# ECOSTRESS Browse Images - Comment out to remove\nfor _n in eco_gdf.index.to_list():\n    folium.raster_layers.ImageOverlay(\n        image=eco_gdf['browse'][_n],\n        name=eco_gdf['granule'][_n],\n        bounds=[[eco_gdf.bounds['miny'][_n], eco_gdf.bounds['minx'][_n]], [eco_gdf.bounds['maxy'][_n], eco_gdf.bounds['maxx'][_n]]],\n        interactive=False,\n        cross_origin=False,\n        opacity=0.75,\n        zindex=1,\n        ).add_to(map1)\n\n# Plot Region of Interest\npolygon.explore(\n    popup=False,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"Region of Interest\",\n    m=map1\n)\n\nfolium.GeoJson(roi_poly,\n                name='bounding_box',\n                ).add_to(map1)\n\nmap1.fit_bounds(bounds=convert_bounds(gdf.unary_union.bounds))\nmap1.add_child(folium.LayerControl())\ndisplay(fig)\n\n\n\n\n\nIn the figure above, you can zoom in and out, click and drag to reposition the legend, and add or remove layers using the layer control in the top right. Notice that since we’re using the tiled ECOSTRESS product, we have 2 overlapping tiles at our ROI. You can visualize the tiles by adding or removing the layers.\nFrom this figure, we can see there are two ECOSTRESS tiles 10SGD, and 10 SKU, which both intersect with our area. Since both fall within tile 10SGD, we can keep results using 10SGD as a filter.\nThere is a lot going on in the above visualization. After doing some additional filtering below, we can re-run the above cell to visualized our filtered scenes.\neco_gdf = eco_gdf[eco_gdf['granule'].str.contains(\"10SGD\")]\n\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')\n\n ECOSTRESS Granules: 15 \n EMIT Granules: 22\n\n\n\n\nThe EMIT browse imagery is not orthorectified, so it can’t be visualized on a plot like the ECOSTRESS browse imagery. To get an idea what scenes look like we can plot them in a grid using matplotlib.\n\nNote: The black space is indicative of onboard cloud masking that occurs before data is downlinked from the ISS.\n\n\ncols = 3\nrows = math.ceil(len(emit_gdf)/cols)\nfig, ax = plt.subplots(rows, cols, figsize=(20,20))\nax = ax.flatten()\n\nfor _n, index in enumerate(emit_gdf.index.to_list()):\n    img = io.imread(emit_gdf['browse'][index])\n    ax[_n].imshow(img)\n    ax[_n].set_title(f\"Index: {index} - {emit_gdf['granule'][index]}\")\n    ax[_n].axis('off')\nplt.tight_layout()\nplt.show\n\n<function matplotlib.pyplot.show(close=None, block=None)>\n\n\n\n\n\n\n\n\nWe can see that some of these granules likely won’t work because of the large amount of cloud cover, we can use a list of these to filter them out. Make a list of indexes to filter out.\n# Bad granule list\nbad_granules = [23,55,88,89,149,155]\nFilter out the bad granules.\nemit_gdf = emit_gdf[~emit_gdf.index.isin(bad_granules)]\nNow that we’ve narrowed our EMIT results we can again filter the ECOSTRESS granules based on their concurrency with our filtered EMIT granules.\n\neco_gdf, emit_gdf = concurrent_match(eco_gdf,emit_gdf, col_name='datetime_obj',time_delta=timedelta(hours=1))\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')\n\n ECOSTRESS Granules: 13 \n EMIT Granules: 16\n\n\nWe can now go back to our folium plot above and re-run the cell to update it based on our filtering.\n\n\n\n\nCreating a list of results URLs will include all of these assets, so if we only want a subset we need an additional filter to keep the specific assets we want.\nIf you look back, you can see we kept the same indexing throughout the notebook. This enables us to simply subset the earthaccess results object to retrieve the results we want.\nCreate a list of index values to keep.\nkeep_granules = eco_gdf.index.to_list()+emit_gdf.index.to_list()\nkeep_granules.sort()\nFilter the results list.\nfiltered_results = [result for i, result in enumerate(results) if i in keep_granules]\nNow we can download all of the associated assets, or retrieve the URLS and further filter them to specifically what we want.\nFirst, log into Earthdata using the login function from the earthaccess library. The persist=True argument will create a local .netrc file if it doesn’t exist, or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them. As mentioned in section 1.2, this step is not necessary to conduct searches, but is needed to download or stream data.\n\nearthaccess.login(persist=True)\n\nEARTHDATA_USERNAME and EARTHDATA_PASSWORD are not set in the current environment, try setting them or use a different strategy (netrc, interactive)\nYou're now authenticated with NASA Earthdata Login\nUsing token with expiration date: 01/12/2024\nUsing .netrc file for EDL\n\n\n<earthaccess.auth.Auth at 0x7f48b0518af0>\n\n\nNow we can download all assets using the following cell.\n# # Download All Assets for Granules in Filtered Results\n# earthaccess.download(filtered_results, '../data/')\nOr we can create a list of URLs and use that to further refine which files we download.\n# Retrieve URLS for Assets\nresults_urls = [granule.data_links() for granule in filtered_results]\n\nresults_urls[:5]\n\n[['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_water.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_cloud.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_height.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_QC.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_LST.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_LST_err.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_EmisWB.tif'],\n ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202939_2305013_002/EMIT_L2A_RFL_001_20230219T202939_2305013_002.nc',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202939_2305013_002/EMIT_L2A_RFLUNCERT_001_20230219T202939_2305013_002.nc',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202939_2305013_002/EMIT_L2A_MASK_001_20230219T202939_2305013_002.nc'],\n ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_water.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_cloud.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_height.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_QC.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_LST.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_LST_err.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_EmisWB.tif'],\n ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202951_2305013_003/EMIT_L2A_RFL_001_20230219T202951_2305013_003.nc',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202951_2305013_003/EMIT_L2A_RFLUNCERT_001_20230219T202951_2305013_003.nc',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202951_2305013_003/EMIT_L2A_MASK_001_20230219T202951_2305013_003.nc'],\n ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_water.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_cloud.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_height.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_QC.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_LST.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_LST_err.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_EmisWB.tif']]\n\n\nWe can see this is a nested list. Granules often have several assets associated with them, for example, ECO_L2T_LSTE has several assets: - Water Mask (water) - Cloud Mask (cloud) - Quality (QC) - Land Surface Temperature (LST) - Land Surface Temperature Error (LST_err) - Wide Band Emissivity (EmisWB) - Height (height)\nThe results list we just generated contains URLs to all of these assets nested by granule. We can further filter our results list using string matching to remove unwanted assets.\nCreate a list of strings and enumerate through our results_url list to filter out unwanted assets and remove the nesting.\n\nfiltered_asset_links = []\n# Pick Desired Assets (leave _ on RFL to distinguish from RFLUNC, LST. to distinguish from LST_err)\ndesired_assets = ['RFL_','MASK', 'LST.'] # Add more or do individually for reflectance, reflectance uncertainty, or mask\n# Step through each sublist (granule) and filter based on desired assets.\nfor n, granule in enumerate(results_urls):\n    for url in granule: \n        asset_name = url.split('/')[-1]\n        if any(asset in asset_name for asset in desired_assets):\n            filtered_asset_links.append(url)\nfiltered_asset_links\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202939_2305013_002/EMIT_L2A_RFL_001_20230219T202939_2305013_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202939_2305013_002/EMIT_L2A_MASK_001_20230219T202939_2305013_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202951_2305013_003/EMIT_L2A_RFL_001_20230219T202951_2305013_003.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202951_2305013_003/EMIT_L2A_MASK_001_20230219T202951_2305013_003.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230227T172116_2305811_004/EMIT_L2A_RFL_001_20230227T172116_2305811_004.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230227T172116_2305811_004/EMIT_L2A_MASK_001_20230227T172116_2305811_004.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_014_10SGD_20230227T172121_0710_01/ECOv002_L2T_LSTE_26345_014_10SGD_20230227T172121_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230401T203751_2309114_002/EMIT_L2A_RFL_001_20230401T203751_2309114_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230401T203751_2309114_002/EMIT_L2A_MASK_001_20230401T203751_2309114_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230401T203803_2309114_003/EMIT_L2A_RFL_001_20230401T203803_2309114_003.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230401T203803_2309114_003/EMIT_L2A_MASK_001_20230401T203803_2309114_003.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26921_001_10SGD_20230405T190258_0710_01/ECOv002_L2T_LSTE_26921_001_10SGD_20230405T190258_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230405T190311_2309513_002/EMIT_L2A_RFL_001_20230405T190311_2309513_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230405T190311_2309513_002/EMIT_L2A_MASK_001_20230405T190311_2309513_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27185_010_10SGD_20230422T195836_0710_01/ECOv002_L2T_LSTE_27185_010_10SGD_20230422T195836_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230422T195924_2311213_002/EMIT_L2A_RFL_001_20230422T195924_2311213_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230422T195924_2311213_002/EMIT_L2A_MASK_001_20230422T195924_2311213_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27745_005_10SGD_20230528T220401_0710_01/ECOv002_L2T_LSTE_27745_005_10SGD_20230528T220401_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230528T220420_2314815_004/EMIT_L2A_RFL_001_20230528T220420_2314815_004.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230528T220420_2314815_004/EMIT_L2A_MASK_001_20230528T220420_2314815_004.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230601T202611_2315214_002/EMIT_L2A_RFL_001_20230601T202611_2315214_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230601T202611_2315214_002/EMIT_L2A_MASK_001_20230601T202611_2315214_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_27806_005_10SGD_20230601T202619_0710_01/ECOv002_L2T_LSTE_27806_005_10SGD_20230601T202619_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_28238_012_10SGD_20230629T170416_0710_01/ECOv002_L2T_LSTE_28238_012_10SGD_20230629T170416_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230629T170449_2318011_002/EMIT_L2A_RFL_001_20230629T170449_2318011_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230629T170449_2318011_002/EMIT_L2A_MASK_001_20230629T170449_2318011_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230923T232101_2326615_002/EMIT_L2A_RFL_001_20230923T232101_2326615_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230923T232101_2326615_002/EMIT_L2A_MASK_001_20230923T232101_2326615_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_29576_005_10SGD_20230923T232104_0710_01/ECOv002_L2T_LSTE_29576_005_10SGD_20230923T232104_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230923T232113_2326615_003/EMIT_L2A_RFL_001_20230923T232113_2326615_003.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230923T232113_2326615_003/EMIT_L2A_MASK_001_20230923T232113_2326615_003.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_29901_007_10SGD_20231014T223936_0711_01/ECOv002_L2T_LSTE_29901_007_10SGD_20231014T223936_0711_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20231014T224006_2328715_002/EMIT_L2A_RFL_001_20231014T224006_2328715_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20231014T224006_2328715_002/EMIT_L2A_MASK_001_20231014T224006_2328715_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20231014T224018_2328715_003/EMIT_L2A_RFL_001_20231014T224018_2328715_003.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20231014T224018_2328715_003/EMIT_L2A_MASK_001_20231014T224018_2328715_003.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20231018T210205_2329114_005/EMIT_L2A_RFL_001_20231018T210205_2329114_005.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20231018T210205_2329114_005/EMIT_L2A_MASK_001_20231018T210205_2329114_005.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_29962_012_10SGD_20231018T210208_0711_01/ECOv002_L2T_LSTE_29962_012_10SGD_20231018T210208_0711_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20231018T210217_2329114_006/EMIT_L2A_RFL_001_20231018T210217_2329114_006.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20231018T210217_2329114_006/EMIT_L2A_MASK_001_20231018T210217_2329114_006.nc']\n\n\nWe can also write this list of files to a text file to have a record of data used, or stream the data using https as we access them. For streaming the data, the EMIT files are very large, so operations can take some time.\nwith open('../data/search_results.txt', 'w') as f:\n    for line in filtered_asset_links:\n        f.write(f\"{line}\\n\")\nUncomment the cell below (select all, then ctrl+/) and download the data that we’ve filtered. Note that this will be a lot (~20GB).\n# # Get requests https Session using Earthdata Login Info\n# fs = earthaccess.get_requests_https_session()\n# # Retrieve granule asset ID from URL (to maintain existing naming convention)\n# for url in filtered_asset_links:\n#     granule_asset_id = url.split('/')[-1]\n#     # Define Local Filepath\n#     fp = f'../data/{granule_asset_id}'\n#     # Download the Granule Asset if it doesn't exist\n#     if not os.path.isfile(fp):\n#         with fs.get(url,stream=True) as src:\n#             with open(fp,'wb') as dst:\n#                 for chunk in src.iter_content(chunk_size=64*1024*1024):\n#                     dst.write(chunk)\nCongratulations, now you have downloaded concurrent data from the ECOSTRESS and EMIT instruments on the ISS.\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 12-01-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.\n\n\n\nThese contain some extra code that may be useful when performing a similar workflow.\n\n\nA large quantity of results may be difficult to understand when mapping with folium. We can create a subset that falls within a single day. First add another column of dates only, then find the unique dates.\n# eco_gdf['date'] = eco_gdf['start_datetime'].str.split('T').str[0]\n# emit_gdf['date'] = emit_gdf['start_datetime'].str.split('T').str[0]\n# emit_gdf['date'].unique()\nFilter both sets of results using a single date.\n# single_day_eco = eco_gdf#[eco_gdf['date'] == '2023-04-23']\n# single_day_emit = emit_gdf#[emit_gdf['date'] == '2023-04-23']\n# print(f' ECOSTRESS Granules: {single_day_eco.shape[0]} \\n EMIT Granules: {single_day_emit.shape[0]}')\n\n\n\nWe can convert a shapefile to a geojson using the following cell. Note that we need to reorder the polygon external vertices so we can submit them as a list of points for our search.\n# # Use Sedgwick Reserve Shapefile\n# # Open Shapefile\n# polygon = gpd.read_file('../data/Sedgwick_Boundary/Sedgwick_Boundary.shp').to_crs(\"EPSG:4326\")\n# # Reorder vertices into Counter-clockwise order\n# polygon.geometry[0] = orient(polygon.geometry[0], sign=1.0)\n# # Save as a geojson (not necessary)\n# polygon.to_file('../data/sedgwick_boundary_epsg4326.geojson', driver='GeoJSON')"
  },
  {
    "objectID": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html",
    "href": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html",
    "title": "VITALS",
    "section": "",
    "text": "Summary\nIn the previous notebook, we found and downloaded concurrent EMIT L2A Reflectance and ECOSTRESS L2 Land Surface Temperature and Emissivity scenes over our region of interest. In this notebook, we will open and explore both datasets to better understand the structure, then we will conduct some common preprocessing routines to make the data usable together, including: applying quality data, reprojecting, placing data on a common grid, and cropping.\n\n\n\nBackground\nThe ECOSTRESS instrument is a multispectral thermal imaging radiometer designed to answer three overarching science questions:\n\nHow is the terrestrial biosphere responding to changes in water availability?\nHow do changes in diurnal vegetation water stress the global carbon cycle?\nCan agricultural vulnerability be reduced through advanced monitoring of agricultural water consumptive use and improved drought estimation?\n\nThe ECOSTRESS mission is answering these questions by accurately measuring the temperature of plants. Plants regulate their temperature by releasing water through tiny pores on their leaves called stomata. If they have sufficient water they can maintain their temperature, but if there is insufficient water, their temperatures rise and this temperature rise can be measured with ECOSTRESS. The images acquired by ECOSTRESS are the most detailed temperature images of the surface ever acquired from space and can be used to measure the temperature of an individual farmers field.\nMore details about ECOSTRESS and its associated products can be found on the ECOSTRESS website and ECOSTRESS product pages hosted by the Land Processes Distributed Active Archive Center (LP DAAC).\nThe EMIT instrument is an imaging spectrometer that measures light in visible and infrared wavelengths. These measurements display unique spectral signatures that correspond to the composition on the Earth’s surface. The EMIT mission focuses specifically on mapping the composition of minerals to better understand the effects of mineral dust throughout the Earth system and human populations now and in the future. In addition, the EMIT instrument can be used in other applications, such as mapping of greenhouse gases, snow properties, and water resources.\nMore details about EMIT and its associated products can be found on the EMIT website and EMIT product pages hosted by the LP DAAC.\nReferences\n- Leith, Alex. 2023. Hyperspectral Notebooks. Jupyter Notebook. Auspatious. https://github.com/auspatious/hyperspectral-notebooks/tree/main\nRequirements\n- NASA Earthdata Account\n- No Python setup requirements if connected to the workshop cloud instance!\n- Local Only - Set up Python Environment. See setup_instructions.md in the /setup/ folder\n- Local Only - Downloaded necessary files. This is done at the end of the 01_Finding_Concurrent_Data notebook.\nLearning Objectives\n- How to open and work with EMIT L2A Reflectance and ECOSTRESS L2T LSTE data\n- How to apply a quality mask to EMIT datasets\n- How to reproject and regrid data\n- How to crop EMIT and ECOSTRESS data\nTutorial Outline\n2.1 Setup\n2.2 Opening and Exploring EMIT Data\n2.2.1 Applying Quality Masks to EMIT Data\n2.2.2 Interactive Plots\n2.2.3 Cropping EMIT Data\n2.2.4 Writing Outputs\n2.3 Opening and Exploring ECOSTRESS Data\n2.3.1 Reprojecting and Regridding ECOSTRESS Data\n2.3.2 Cropping ECOSTRESS Data\n2.3.3 Writing Outputs\n\n\nImport Python libraries.\n# Import Packages\nimport os\nimport csv\nimport warnings\nimport glob\nimport math\nimport earthaccess\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray as rxr\nimport hvplot.xarray\nimport hvplot.pandas\nimport holoviews as hv\nimport geoviews as gv\nimport geopandas as gp\nimport sys\nfrom modules.emit_tools import emit_xarray, ortho_xr\nfrom holoviews.plotting.links import DataLink\nDefine a filepath for an EMIT L2A Reflectance file, EMIT L2A Mask file, and an ECOSTRESS L2T LSTE and ECOSTRESS L2T Mask file. The files selected in this example are from April 1, 2023 at around 20:37.\nemit_fp = \"../../shared/2023-VITALS-Workshop-AGU/data/EMIT_L2A_RFL_001_20230401T203751_2309114_002.nc\"\nemit_qa_fp = \"../../shared/2023-VITALS-Workshop-AGU/data/EMIT_L2A_MASK_001_20230401T203751_2309114_002.nc\"\neco_fp = \"../../shared/2023-VITALS-Workshop-AGU/data/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif\"\n\n\n\nEMIT L2A Reflectance Data are distributed in a non-orthocorrected spatially raw NetCDF4 (.nc) format consisting of the data and its associated metadata. Inside the L2A Reflectance .nc file there are 3 groups. Groups can be thought of as containers to organize the data.\n\nThe root group that can be considered the main dataset contains the reflectance data described by the downtrack, crosstrack, and bands dimensions.\n\nThe sensor_band_parameters group containing the wavelength center and the full-width half maximum (FWHM) of each band.\n\nThe location group contains latitude and longitude values at the center of each pixel described by the crosstrack and downtrack dimensions, as well as a geometry lookup table (GLT) described by the ortho_x and ortho_y dimensions. The GLT is an orthorectified image (EPSG:4326) consisting of 2 layers containing downtrack and crosstrack indices. These index positions allow us to quickly project the raw data onto this geographic grid.\n\nTo work with the EMIT data, we will use the emit_tools module. There are other ways to work with the data and a more thorough explanation of the emit_tools in the EMIT-Data-Resources Repository.\nOpen the example EMIT scene using the emit_xarray function. In this step we will use the ortho=True argument to orthorectify the scene using the included GLT.\nemit_ds = emit_xarray(emit_fp, ortho=True)\nemit_ds\nWe can plot the spectra of an individual pixel closest to a latitude and longitude we want using the sel function from xarray. Using the good_wavelengths flag from the sensor_band_parameters group, mask out bands where water absorption features were assigned a value of -0.01 reflectance. Typically data around 1320-1440 nm and 1770-1970 nm is noisy due to the moisture present in the atmosphere; therefore, these spectral regions offer little information about targets and can be excluded from calculations.\nemit_ds['reflectance'].data[:,:,emit_ds['good_wavelengths'].data==0] = np.nan\nNow select a point and plot a spectra. In this example, we’ll first find the center of the scene and use those coordinates.\nscene_center = emit_ds.latitude.values[int(len(emit_ds.latitude)/2)],emit_ds.longitude.values[int(len(emit_ds.longitude)/2)]\nscene_center\npoint = emit_ds.sel(latitude=scene_center[0],longitude=scene_center[1], method='nearest')\npoint.hvplot.line(y='reflectance',x='wavelengths', color='black').opts(\n    title=f'Latitude = {point.latitude.values.round(3)}, Longitude = {point.longitude.values.round(3)}')\nWe can also plot individual bands spatially by selecting a wavelength, then plotting. Select the band with a wavelengths of 850 nm and plot it using ESRI imagery as a basemap to get a better understanding of where the scene was acquired.\nemit_layer = emit_ds.sel(wavelengths=850,method='nearest')\nemit_layer.hvplot.image(cmap='viridis',geo=True, tiles='ESRI', frame_width=720,frame_height=405, alpha=0.7, fontscale=2).opts(\n    title=f\"{emit_layer.wavelengths:.3f} {emit_layer.wavelengths.units}\", xlabel='Longitude',ylabel='Latitude')\n\n\nThe EMIT L2A Mask file contains some bands that are direct masks (Cloud, Dilated, Cirrus, Water, Spacecraft), and some (AOD550 and H2O (g cm-2)) that contain information calculated during the L2A reflectance retrieval. These may be used as additional screening, depending on the application. The Aggregate Flag is the mask used during EMIT L2B Mineralogy calculations, which we will also use here, but not all users might want this particular mask.\n\nNote: It is more memory efficient to apply the mask before orthorectifying, so during the automation section we will do that.\n\nemit_mask = emit_xarray(emit_qa_fp, ortho=True)\nemit_mask\nList the quality flags contained in the mask_bands dimension.\nemit_mask.mask_bands.data.tolist()\nAs mentioned, we will use the Aggregate Flag. Select that band with the sel function as we did for wavelengths before.\nemit_aggregate_mask = emit_mask.sel(mask_bands='Aggregate Flag')\nNow we can visualize our aggregate quality mask. You may have noticed before that we added a lot of parameters to our plotting function. If we want to consistently apply the same formatting for multiple plots, we can add those agrguments to a dictionary that we can unpack into hvplot functions using **.\nCreate two dictionaries with plotting options.\nsize_opts = dict(frame_height=405, frame_width=720, fontscale=2)\nmap_opts = dict(geo=True,alpha=0.7, xlabel='Longitude',ylabel='Latitude')\nemit_aggregate_mask.hvplot.image(cmap='viridis', tiles='ESRI', **size_opts, **map_opts)\nValues of 1 in the mask indicate areas to omit. Apply the mask to our EMIT Data by assigning values where the mask.data == 1 to np.nan\nemit_ds.reflectance.data[emit_aggregate_mask.mask.data == 1] = np.nan\nWe can confirm our masking worked with a spatial plot.\nemit_layer_filtered_plot = emit_ds.sel(wavelengths=850, method='nearest').hvplot.image(cmap='viridis',tiles='ESRI',**size_opts, **map_opts)\nemit_layer_filtered_plot\n\n\n\nCombining the Spatial and Spectral information into a single visualization can be a powerful tool for exploring and inspecting imaging spectroscopy data. Using the streams module from Holoviews we can link a spatial map to a plot of spectra.\nWe could plot a single band image as we previously have, but using a multiband image, like an RGB may help infer what targets we’re examining. Build an RGB image following the steps below.\nSelect bands to represent red (650 nm), green (560 nm), and blue (470 nm) by finding the nearest to a wavelength chosen to represent that color.\nemit_rgb = emit_ds.sel(wavelengths=[650, 560, 470], method='nearest')\nWe may need to adjust balance the brightness of the selected wavelengths to make a prettier map. This will not affect the data, just the visuals. To do this we will use the function below. We can change the bright argument to increase or decrease the brightness of the scene as a whole. A value of 0.2 usually works pretty well.\ndef gamma_adjust(rgb_ds, bright=0.2, white_background=False):\n    array = rgb_ds.reflectance.data\n    gamma = math.log(bright)/math.log(np.nanmean(array)) # Create exponent for gamma scaling - can be adjusted by changing 0.2 \n    scaled = np.power(np.nan_to_num(array,nan=1),np.nan_to_num(gamma,nan=1)).clip(0,1) # Apply scaling and clip to 0-1 range\n    if white_background == True:\n        scaled = np.nan_to_num(scaled, nan = 1) # Assign NA's to 1 so they appear white in plots\n    rgb_ds.reflectance.data = scaled\n    return rgb_ds\n# Suppress Warnings for Some of the holoviews plots\nwarnings.filterwarnings('ignore')\nemit_rgb = gamma_adjust(emit_rgb,white_background=True)\nNow that we have an RGB dataset, we can use that to create a spatial plot, and data selected by clicking on that ‘map’ can be inputs for a function to return values from the full dataset at that latitude and longitude location using the cell below. To visualize the spectral and spatial data side-by-side, we use the Point Draw tool from the holoviews library.\nDefine a limit to the quantity of points and spectra we will plot, a list of colors to cycle through, and an initial point. Then use the input from the Tap function to provide clicked x and y positions on the map and use these to retrieve spectra from the dataset at those coordinates.\nClick in the RGB image to add spectra to the plot. You can also click and hold the mouse button then drag previously place points. To remove a point click and hold the mouse button down, then press the backspace key.\n# Interactive Points Plotting\n# Modified from https://github.com/auspatious/hyperspectral-notebooks/blob/main/03_EMIT_Interactive_Points.ipynb\nPOINT_LIMIT = 10\ncolor_cycle = hv.Cycle('Category20')\n\n# Create RGB Map\nmap = emit_rgb.hvplot.rgb(fontscale=1.5, xlabel='Longitude',ylabel='Latitude',frame_width=480, frame_height=480)\n\n# Set up a holoviews points array to enable plotting of the clicked points\nxmid = emit_ds.longitude.values[int(len(emit_ds.longitude) / 2)]\nymid = emit_ds.latitude.values[int(len(emit_ds.latitude) / 2)]\n\nfirst_point = ([xmid], [ymid], [0])\npoints = hv.Points(first_point, vdims='id')\npoints_stream = hv.streams.PointDraw(\n    data=points.columns(),\n    source=points,\n    drag=True,\n    num_objects=POINT_LIMIT,\n    styles={'fill_color': color_cycle.values[1:POINT_LIMIT+1], 'line_color': 'gray'}\n)\n\nposxy = hv.streams.PointerXY(source=map, x=xmid, y=ymid)\nclickxy = hv.streams.Tap(source=map, x=xmid, y=ymid)\n\n# Function to build spectral plot of clicked location to show on hover stream plot\ndef click_spectra(data):\n    coordinates = []\n    if data is None or not any(len(d) for d in data.values()):\n        coordinates.append(clicked_points[0][0], clicked_points[1][0])\n    else:\n        coordinates = [c for c in zip(data['x'], data['y'])]\n    \n    plots = []\n    for i, coords in enumerate(coordinates):\n        x, y = coords\n        data = emit_ds.sel(longitude=x, latitude=y, method=\"nearest\")\n        plots.append(\n            data.hvplot.line(\n                y=\"reflectance\",\n                x=\"wavelengths\",\n                color=color_cycle,\n                label=f\"{i}\"\n            )\n        )\n        points_stream.data[\"id\"][i] = i\n    return hv.Overlay(plots)\n\ndef hover_spectra(x,y):\n    return emit_ds.sel(longitude=x,latitude=y,method='nearest').hvplot.line(y='reflectance',x='wavelengths',\n                                                                            color='black', frame_width=400)\n# Define the Dynamic Maps\nclick_dmap = hv.DynamicMap(click_spectra, streams=[points_stream])\nhover_dmap = hv.DynamicMap(hover_spectra, streams=[posxy])\n# Plot the Map and Dynamic Map side by side\nhv.Layout(hover_dmap*click_dmap + map * points).cols(2).opts(\n    hv.opts.Points(active_tools=['point_draw'], size=10, tools=['hover'], color='white', line_color='gray'),\n    hv.opts.Overlay(show_legend=False, show_title=False, fontscale=1.5, frame_height=480)\n)\nWe can take these selected points and the corresponding reflectance spectra and save them as a .csv for later use.\nSelect 10 points by adding to the figure above. We will save these and use them in a to caclculate Equivalent Water Thickness or Canopy water content in the next notebook.\nAfter making a selection, re-enable warnings to make sure we don’t have issues with other cells in the notebook.\n# Enable Warnings again\nwarnings.filterwarnings('default')\nBuild a dictionary of the selected points and spectra, then export the spectra to a .csv file.\ndata = points_stream.data\nwavelengths = emit_ds.wavelengths.values\n\nrows = [[\"id\", \"x\", \"y\"] + [str(i) for i in wavelengths]]\n \nfor p in zip(data['x'], data['y'], data['id']):\n    x, y, i = p\n    spectra = emit_ds.sel(longitude=x, latitude=y, method=\"nearest\").reflectance.values\n    row = [i, x, y] + list(spectra)\n    rows.append(row)\nWe’ve preselected 10 points, but feel free to uncomment the cell below to use your own. This will overwrite the file containing the preselected points.\n# with open('../data/emit_click_data.csv', 'w', newline='') as f:\n#     writer = csv.writer(f)\n#     writer.writerows(rows)\n\n\n\nTo crop our dataset to our ROI we first need to open a shapefile of the region. Open the included geojson for Sedgwick Reserve and Plot it onto our EMIT 850nm reflectance spatial plot. Note that here we don’t use use tiles – there seems to be a bug preventing this.\nshape = gp.read_file(\"../data/dangermond_boundary.geojson\")\nshape\nemit_ds.sel(wavelengths=850, method='nearest').hvplot.image(cmap='viridis',**size_opts,**map_opts,tiles='ESRI')*shape.to_crs('EPSG:3857').hvplot(color='#d95f02',alpha=0.5)\nNow use the clip function from rasterio to crop the data to our ROI using our shape’s geometry and crs. The all_touched=True argument will ensure all pixels touched by our polygon will be included.\nemit_cropped = emit_ds.rio.clip(shape.geometry.values,shape.crs, all_touched=True)\nPlot the cropped data.\nemit_cropped.sel(wavelengths=850,method='nearest').hvplot.image(cmap='viridis', tiles='ESRI', **size_opts, **map_opts)\n\n\n\nLastly for our EMIT dataset, we can write a smaller output that we can use in later notebooks, to calculate Canopy water content or other applications. We use the granule_id from the dataset to keep a similar naming convention.\n# Write Clipped Output\n#emit_cropped.to_netcdf(f'../data/{emit_cropped.granule_id}_dangermond.nc')\n\n\n\n\nFor this example we’re only taking a look at the ECOSTRESS Level 2 Tiled Land Surface Temperature (ECO_L2T_LSTE). The Land Surface Temperature and Emissivity values are derived from five thermal infrared (TIR) bands using a physics-based Temperature and Emissivity Separation (TES) algorithm. This tiled data product uses a modified version of the Military Grid Reference System (MGRS) which divides Universal Transverse Mercator (UTM) zones into square tiles that are 109.8 km by 109.8 km with a 70 meter (m) spatial resolution.\nOpen the LSTE file using open_rasterio from the rioxarray library. Since the file consists of only 1 layer, we can squeeze it, removing the band dimension.\neco_lst_ds = rxr.open_rasterio(eco_fp).squeeze('band', drop=True)\neco_lst_ds\nAs mentioned the ECOSTRESS product we are using here is tiled and the coordinate reference system (CRS) is dependent on UTM zone. For this tile, we can look at the spatial_ref variable through the interactive object above to see details such as the well-known-text (WKT) representation of the CRS and other attributes.\nNow lets plot the data using hvplot. The underlying functions will recognize the CRS and reprojet the scene for our visualization with an ESRI imagery RBG background tile.\neco_lst_ds.hvplot.image(x='x',y='y',**size_opts, cmap='inferno',tiles='ESRI', xlabel='Longitude',ylabel='Latitude',title='ECOSTRESS LST (K)')\n\n\nWe will need to reproject manually to pair this scene with the EMIT data, but we will not need to mask ECOSTRESS, because cloud masking has already been done for the tiled LSTE product.\nTo give a reasonable 1:1 comparison, in addition to reprojecting we want the data on the same grid, so each pixel from the ECOSTRESS scene corresponds to a pixel in the EMIT scene. To do this, we can use the reproject_match function from the rioxarray library. This will reproject and regrid our ECOSTRESS data to match the EMIT CRS and grid using the spatial_ref variable from each dataset. Since we’ve already cropped the EMIT scene, this will limit our ECOSTRESS scene to the extent of that cropped EMIT scene as well.\neco_lst_ds_regrid = eco_lst_ds.rio.reproject_match(emit_cropped)\nWe can now visualize our reprojected, regridded ECOSTRESS LSTE scene.\neco_lst_ds_regrid.hvplot.image(geo=True, tiles='ESRI',cmap='inferno',**size_opts, xlabel='Longitude',ylabel='Latitude',title='Regridded ECOSTRESS LST (K)')\n\n\n\nThis has been cropped to the extent, but we can further mask data outside of our region of interest by using the clip function like we did for EMIT data.\neco_dangermond = eco_lst_ds_regrid.rio.clip(shape.geometry.values,shape.crs, all_touched=True)\neco_dangermond.hvplot.image(geo=True,cmap='inferno',**size_opts, tiles='ESRI',alpha=0.7, title='Cropped ECOSTRESS LST (K)', xlabel='Longitude',ylabel='Latitude')\n\n\n\nWe now have a subset ECOSTRESS scene that is aligned with EMIT data that we can export for our use in later notebooks.\nSave the ECOSTRESS LSTE scene.\n# Uncomment to overwrite included sample\n# eco_outname = f\"../data/{eco_fp.split('/')[-1].split('.')[0]}_dangermond.tif\"\n# eco_sedgwick.rio.to_raster(raster_path=eco_outname, driver='COG')\n\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 11-28-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "python/03_EMIT_CWC_from_Reflectance.html",
    "href": "python/03_EMIT_CWC_from_Reflectance.html",
    "title": "VITALS",
    "section": "",
    "text": "Summary\nIn this notebook we will explore how Equivalent Water Thickness (EWT) or Canopy Water Content (CWC) can be calculated from the Earth Surface Mineral Dust Source Investigation (EMIT) L2A Reflectance Product, then we will apply this knowledge to calculate CWC over the Jack and Laura Dangermond Preserve located near Santa Barbara, CA.\n\n\n\nBackground\nEquivalent Water Thickness (EWT) is the predicted thickness or absorption path length in centimeters (cm) of water that would be required to yield an observed spectra. In the context of vegetation, this is equivalent to canopy water content (CWC) in g/cm^2 because a cm^3 of water has a mass of 1g.\nCWC can be derived from surface reflectance spectra because they provide information about the composition of the target, including water content. Reflectance is the fraction of incoming solar radiation reflected by Earth’s surface. Different materials reflect varying proportions of radiation based upon their chemical composition and physical properties, giving materials their own unique spectral signature or fingerprint. In particular, liquid water causes characteristic absorption features to appear in the near-infrared wavelengths of the solar spectrum, which enables an estimation of its content.\nCWC correlates with vegetation type and health, as well as wildfire risk. The methods used here to calculate CWC are based on the ISOFIT python package. The Beer-Lambert physical model used to calculate CWC is described in Green et al. (2006) and Bohn et al. (2020). It uses wavelength-dependent absorption coefficients of liquid water to determine the absorption path length as a function of absorption feature depth. Of note, this model does not account for multiple scattering effects within the canopy and may result in oversestimation of CWC (Bohn et al., 2020).\nThe Jack and Laura Dangermond Preserve and its surrounding lands are one one of the last “wild coastal” regions in Southern California. The preserve is over 24,000 acres and consists of several ecosystem types and is home to over 600 plant species and over 200 wildlife species.\nMore about the EMIT mission and EMIT products.\nReferences\n\nShrestha, Rupesh. 2023. Equivalent water thickness/canopy water content from hyperspectral data. Jupyter Notebook. Oak Ridge National Laboratory Distributed Active Archive Center. https://github.com/rupesh2/ewt_cwc/tree/main\nBohn, N., L. Guanter, T. Kuester, R. Preusker, and K. Segl. 2020. Coupled retrieval of the three phases of water from spaceborne imaging spectroscopy measurements. Remote Sensing of Environment 242:111708. https://doi.org/10.1016/j.rse.2020.111708\nGreen, R.O., T.H. Painter, D.A. Roberts, and J. Dozier. 2006. Measuring the expressed abundance of the three phases of water with an imaging spectrometer over melting snow. Water Resources Research 42:W10402. https://doi.org/10.1029/2005WR004509\nThompson, D.R., V. Natraj, R.O. Green, M.C. Helmlinger, B.-C. Gao, and M.L. Eastwood. 2018. Optimal estimation for imaging spectrometer atmospheric correction. Remote Sensing of Environment 216:355–373. https://doi.org/10.1016/j.rse.2018.07.003\n\nRequirements - NASA Earthdata Account\n- No Python setup requirements if connected to the workshop cloud instance!\n- Local Only - Set up Python Environment. See setup_instructions.md in the /setup/ folder\n- Local Only - Downloaded necessary files. This is done at the end of the 01_Finding_Concurrent_Data notebook.\nLearning Objectives\n- Calculate CWC of a single pixel\n- Calculate CWC of an ROI\nTutorial Outline\n3.1 Setup\n3.2 Opening EMIT Data\n3.3 Extracting Reflectance of a Pixel\n3.4 Calculating CWC\n3.4.1 Single Point\n3.4.2 DataFrame of Points\n3.5 Applying Inversion in Parallel Across an ROI\n\n# Install ray on 2i2c\n!pip install \"ray[default]\"\n\nRequirement already satisfied: ray[default] in /srv/conda/envs/notebook/lib/python3.10/site-packages (2.8.1)\nRequirement already satisfied: click>=7.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (8.1.7)\nRequirement already satisfied: filelock in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (3.12.4)\nRequirement already satisfied: jsonschema in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (4.19.1)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (1.0.5)\nRequirement already satisfied: packaging in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (23.1)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (4.25.1)\nRequirement already satisfied: pyyaml in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (6.0.1)\nRequirement already satisfied: aiosignal in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (1.3.1)\nRequirement already satisfied: frozenlist in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (1.4.0)\nRequirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (2.31.0)\nRequirement already satisfied: numpy>=1.19.3 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (1.24.4)\nRequirement already satisfied: aiohttp>=3.7 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (3.8.5)\nRequirement already satisfied: aiohttp-cors in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (0.7.0)\nRequirement already satisfied: colorful in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (0.5.5)\nRequirement already satisfied: py-spy>=0.2.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (0.3.14)\nRequirement already satisfied: gpustat>=1.0.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (1.1.1)\nRequirement already satisfied: opencensus in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (0.11.3)\nRequirement already satisfied: pydantic<2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (1.10.13)\nRequirement already satisfied: prometheus-client>=0.7.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (0.17.1)\nRequirement already satisfied: smart-open in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (6.4.0)\nRequirement already satisfied: virtualenv<20.21.1,>=20.0.24 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (20.21.0)\nRequirement already satisfied: grpcio>=1.42.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from ray[default]) (1.59.3)\nRequirement already satisfied: attrs>=17.3.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]) (1.9.2)\nRequirement already satisfied: nvidia-ml-py>=11.450.129 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[default]) (12.535.133)\nRequirement already satisfied: psutil>=5.6.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[default]) (5.9.5)\nRequirement already satisfied: blessed>=1.17.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[default]) (1.20.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pydantic<2->ray[default]) (4.8.0)\nRequirement already satisfied: distlib<1,>=0.3.6 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from virtualenv<20.21.1,>=20.0.24->ray[default]) (0.3.7)\nRequirement already satisfied: platformdirs<4,>=2.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from virtualenv<20.21.1,>=20.0.24->ray[default]) (3.10.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from jsonschema->ray[default]) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from jsonschema->ray[default]) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from jsonschema->ray[default]) (0.10.3)\nRequirement already satisfied: opencensus-context>=0.1.3 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from opencensus->ray[default]) (0.1.3)\nRequirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from opencensus->ray[default]) (2.14.0)\nRequirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->ray[default]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->ray[default]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from requests->ray[default]) (2023.7.22)\nRequirement already satisfied: wcwidth>=0.1.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]) (0.2.6)\nRequirement already satisfied: six>=1.9.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]) (1.16.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (1.61.0)\nRequirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (2.25.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (5.3.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (4.9)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /srv/conda/envs/notebook/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.5.1)\n\n\n\n\nImport the required Python libraries.\n\n# Import Packages\nimport os\nimport glob\nimport earthaccess\nimport math\nimport numpy as np\nimport xarray as xr\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray as rxr\nfrom matplotlib import pyplot as plt\nimport hvplot.xarray\nimport hvplot.pandas\nimport holoviews as hv\nimport pandas as pd\nimport geopandas as gp\nimport sys\nfrom modules.emit_tools import emit_xarray, ortho_xr\nfrom modules.ewt_calc import calc_ewt\nfrom scipy.optimize import least_squares\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEMIT L2A Reflectance Data are distributed in a non-orthocorrected spatially raw NetCDF4 (.nc) format consisting of the data and its associated metadata. To work with this data, we will use the emit_xarray function from the emit_tools.py module included in the repository.\nSet a filepath for an EMIT L2A reflectance file and open using the emit_xarray function.\nfp = '../../shared/2023-VITALS-Workshop-AGU/data/EMIT_L2A_RFL_001_20230401T203751_2309114_002.nc'\nds = emit_xarray(fp,ortho=True)\nSimilarly to what we did in the previous notebook, we can plot a single band to get an idea of where our scene is. This is the same scene we processed earlier.\n\nemit_layer = ds.sel(wavelengths=850,method='nearest')\nemit_layer.hvplot.image(cmap='viridis',geo=True, tiles='ESRI', frame_width=720,frame_height=405, alpha=0.7, fontscale=2).opts(\n    title=f\"{emit_layer.wavelengths:.3f} {emit_layer.wavelengths.units}\", xlabel='Longitude',ylabel='Latitude')\n\nUnable to display output for mime type(s): \n\n\n\n  \n\n\n\n\n\n\n\nWe can mask out the -.01 values used to represent the region of the spectra with strong atmospheric water vapor absorption features.\nds['reflectance'].data[:,:,ds['good_wavelengths'].data==0] = np.nan\nRetrieve the spectra from a sample point by providing a latitude and longitude along with a method using the sel function. This will select the pixel closest to the provided coordinates.\n\npoint = ds.sel(latitude=34.5399,longitude=-120.3529, method='nearest')\npoint\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (wavelengths: 285)\nCoordinates:\n  * wavelengths       (wavelengths) float32 381.0 388.4 ... 2.486e+03 2.493e+03\n    fwhm              (wavelengths) float32 ...\n    good_wavelengths  (wavelengths) float32 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0\n    latitude          float64 34.54\n    longitude         float64 -120.4\n    elev              float32 130.3\n    spatial_ref       int64 0\nData variables:\n    reflectance       (wavelengths) float32 0.01718 0.01754 ... 0.02635 0.02455\nAttributes: (12/40)\n    ncei_template_version:             NCEI_NetCDF_Swath_Template_v2.0\n    summary:                           The Earth Surface Mineral Dust Source ...\n    keywords:                          Imaging Spectroscopy, minerals, EMIT, ...\n    Conventions:                       CF-1.63\n    sensor:                            EMIT (Earth Surface Mineral Dust Sourc...\n    instrument:                        EMIT\n    ...                                ...\n    spatial_ref:                       GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHER...\n    geotransform:                      [-1.20992414e+02  5.42232520e-04 -0.00...\n    day_night_flag:                    Day\n    title:                             EMIT L2A Estimated Surface Reflectance...\n    granule_id:                        EMIT_L2A_RFL_001_20230401T203751_23091...\n    Orthorectified:                    Truexarray.DatasetDimensions:wavelengths: 285Coordinates: (7)wavelengths(wavelengths)float32381.0 388.4 ... 2.486e+03 2.493e+03long_name :Wavelength Centersunits :nmarray([ 381.00558,  388.4092 ,  395.81583, ..., 2478.153  , 2485.5386 ,\n       2492.9238 ], dtype=float32)fwhm(wavelengths)float32...long_name :Full Width at Half Maxunits :nm[285 values with dtype=float32]good_wavelengths(wavelengths)float321.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0long_name :Wavelengths where reflectance is useable: 1 = good data, 0 = bad dataunits :unitlessarray([1., 1., 1., ..., 1., 1., 1.], dtype=float32)latitude()float6434.54long_name :Latitude (WGS-84)units :degrees northarray(34.5401589)longitude()float64-120.4long_name :Longitude (WGS-84)units :degrees eastarray(-120.35285082)elev()float32130.3long_name :Surface Elevationunits :marray(130.26474, dtype=float32)spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]array(0)Data variables: (1)reflectance(wavelengths)float320.01718 0.01754 ... 0.02635 0.02455long_name :Surface Reflectanceunits :unitlessarray([0.01718401, 0.01754001, 0.01789793, 0.01825578, 0.01887722,\n       0.01977932, 0.02089375, 0.02213017, 0.02327097, 0.02443924,\n       0.02553891, 0.02647934, 0.02731651, 0.02806388, 0.03012716,\n       0.0303051 , 0.03147712, 0.03389502, 0.03716784, 0.04125404,\n       0.04527088, 0.04904396, 0.0513751 , 0.05365279, 0.05416293,\n       0.05240294, 0.05057795, 0.04905905, 0.04850443, 0.04918665,\n       0.04959129, 0.0485189 , 0.04756864, 0.04709126, 0.04778447,\n       0.04722033, 0.04627838, 0.04591437, 0.04385856, 0.04450385,\n       0.04487438, 0.04721367, 0.05855059, 0.07728622, 0.09845995,\n       0.11936122, 0.14146721, 0.16280495, 0.17837006, 0.1891758 ,\n       0.19495177, 0.19879304, 0.20209542, 0.20503408, 0.20749208,\n       0.20980741, 0.21209788, 0.21429807, 0.2161592 , 0.21820003,\n       0.2202218 , 0.2222354 , 0.22431302, 0.22625907, 0.22809783,\n       0.2298785 , 0.23154566, 0.23314181, 0.23455934, 0.2356244 ,\n       0.23671815, 0.2378939 , 0.2389565 , 0.23990753, 0.23933375,\n       0.238821  , 0.23855045, 0.23721987, 0.23602907, 0.23627467,\n       0.23715375, 0.2386118 , 0.2409895 , 0.24408886, 0.24704838,\n       0.25001273, 0.25301102, 0.25599742, 0.258825  , 0.26146266,\n       0.26378876, 0.2659258 , 0.2676973 , 0.26931012, 0.27055526,\n       0.27149647, 0.2718703 , 0.27186936, 0.2714865 , 0.2693018 ,\n...\n              nan,        nan,        nan,        nan,        nan,\n              nan,        nan,        nan,        nan,        nan,\n              nan,        nan,        nan,        nan,        nan,\n              nan,        nan,        nan,        nan,        nan,\n              nan,        nan,        nan, 0.05113549, 0.05279826,\n       0.05452276, 0.05609974, 0.05744791, 0.05934713, 0.06077929,\n       0.06164304, 0.06214674, 0.06242346, 0.06241743, 0.06247758,\n       0.06254932, 0.062612  , 0.06495836, 0.06524801, 0.06746189,\n       0.06759153, 0.06831566, 0.06865513, 0.06930527, 0.07104894,\n       0.07243683, 0.07320293, 0.07459541, 0.07555467, 0.07578479,\n       0.07651836, 0.07675724, 0.07744075, 0.07785628, 0.0794607 ,\n       0.07826924, 0.0792022 , 0.07868779, 0.07894044, 0.07777485,\n       0.07542186, 0.07294796, 0.07028439, 0.06905952, 0.06607912,\n       0.06531868, 0.06337301, 0.0620889 , 0.05987899, 0.05752026,\n       0.05735842, 0.05715922, 0.05624693, 0.05384592, 0.05267731,\n       0.05235471, 0.05194782, 0.05061792, 0.05089531, 0.05007195,\n       0.04892436, 0.04694235, 0.04603988, 0.0461389 , 0.04569674,\n       0.0415682 , 0.04030755, 0.03803281, 0.03684125, 0.03320232,\n       0.03311669, 0.03205504, 0.02982526, 0.0263453 , 0.02454741],\n      dtype=float32)Indexes: (1)wavelengthsPandasIndexPandasIndex(Float64Index([ 381.0055847167969,  388.4092102050781,  395.8158264160156,\n              403.22540283203125, 410.63800048828125,  418.0535888671875,\n               425.4721374511719,  432.8927001953125,  440.3172607421875,\n               447.7427978515625,\n              ...\n               2426.440185546875,  2433.830322265625,   2441.21826171875,\n                 2448.6064453125,  2455.994384765625,  2463.381591796875,\n               2470.767822265625,  2478.153076171875,   2485.53857421875,\n                  2492.923828125],\n             dtype='float64', name='wavelengths', length=285))Attributes: (40)ncei_template_version :NCEI_NetCDF_Swath_Template_v2.0summary :The Earth Surface Mineral Dust Source Investigation (EMIT) is an Earth Ventures-Instrument (EVI-4) Mission that maps the surface mineralogy of arid dust source regions via imaging spectroscopy in the visible and short-wave infrared (VSWIR). Installed on the International Space Station (ISS), the EMIT instrument is a Dyson imaging spectrometer that uses contiguous spectroscopic measurements from 410 to 2450 nm to resolve absoprtion features of iron oxides, clays, sulfates, carbonates, and other dust-forming minerals. During its one-year mission, EMIT will observe the sunlit Earth's dust source regions that occur within +/-52° latitude and produce maps of the source regions that can be used to improve forecasts of the role of mineral dust in the radiative forcing (warming or cooling) of the atmosphere.\\n\\nThis file contains L2A estimated surface reflectances and geolocation data. Reflectance estimates are created using an Optimal Estimation technique - see ATBD for details. Reflectance values are reported as fractions (relative to 1). Geolocation data (latitude, longitude, height) and a lookup table to project the data are also included.keywords :Imaging Spectroscopy, minerals, EMIT, dust, radiative forcingConventions :CF-1.63sensor :EMIT (Earth Surface Mineral Dust Source Investigation)instrument :EMITplatform :ISSinstitution :NASA Jet Propulsion Laboratory/California Institute of Technologylicense :https://science.nasa.gov/earth-science/earth-science-data/data-information-policy/naming_authority :LPDAACdate_created :2023-04-06T02:26:24Zkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstdname_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventioncreator_name :Jet Propulsion Laboratory/California Institute of Technologycreator_url :https://earth.jpl.nasa.gov/emit/project :Earth Surface Mineral Dust Source Investigationproject_url :https://emit.jpl.nasa.gov/publisher_name :NASA LPDAACpublisher_url :https://lpdaac.usgs.govpublisher_email :lpdaac@usgs.govidentifier_product_doi_authority :https://doi.orgflight_line :emit20230401t203751_o09114_s000time_coverage_start :2023-04-01T20:37:51+0000time_coverage_end :2023-04-01T20:38:03+0000software_build_version :010610software_delivery_version :010610product_version :V001history :PGE Run Command: {python /beegfs/store/emit/ops/repos/emit-sds-l2a/spectrum_quality.py /tmp/emit/ops/emit20230401t203751_emit.L2AReflectance_20230405t233102/output/emit20230401t203751_rfl /tmp/emit/ops/emit20230401t203751_emit.L2AReflectance_20230405t233102/output/emit20230401t203751_rfl_quality.txt}, PGE Input Files: {radiance_file=/beegfs/store/emit/ops/data/acquisitions/20230401/emit20230401t203751/l1b/emit20230401t203751_o09114_s000_l1b_rdn_b0106_v01.img, pixel_locations_file=/beegfs/store/emit/ops/data/acquisitions/20230401/emit20230401t203751/l1b/emit20230401t203751_o09114_s000_l1b_loc_b0106_v01.img, observation_parameters_file=/beegfs/store/emit/ops/data/acquisitions/20230401/emit20230401t203751/l1b/emit20230401t203751_o09114_s000_l1b_obs_b0106_v01.img, surface_model_config=/beegfs/store/emit/ops/repos/emit-sds-l2a/surface/surface_20221020.json}crosstrack_orientation :as seen on groundeasternmost_longitude :-119.71545648976226northernmost_latitude :34.7990749308955westernmost_longitude :-120.992414074966southernmost_latitude :33.77100207248943spatialResolution :0.000542232520256367spatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]geotransform :[-1.20992414e+02  5.42232520e-04 -0.00000000e+00  3.47990749e+01\n -0.00000000e+00 -5.42232520e-04]day_night_flag :Daytitle :EMIT L2A Estimated Surface Reflectance 60 m V001granule_id :EMIT_L2A_RFL_001_20230401T203751_2309114_002Orthorectified :True\n\n\nWe can plot this information to see the spectra.\n\npoint.hvplot.line(x='wavelengths',y='reflectance',color='black').opts(title=f\"Latitude: {point.latitude.values:.3f} Longitude: {point.longitude.values:.3f}\")\n\nUnable to display output for mime type(s): \n\n\n\n  \n\n\n\n\n\n\n\nAs mentioned in the background we use the surface reflectance to estimate CWC. The unique spectral signatures allow identification and quantification based upon the wavelength-dependent absorption coefficients of liquid water. The EMIT mission has applied similar approaches to identify dust source minerals as well as methane point source emissions. The path length of liquid water absorption can be estimated by utilizing a least squares inversion to minimize the residuals between the EMIT reflectance and the Beer-Lambert model (Green et al.,2006), which relates the wavelength-dependent absorption to the path length the photons are traveling through the material. During the inversion, the path lengths are iteratively adjusted to match the modeled spectra to the EMIT reflectance within the water absorption feature region from 850 to 1100 nm.\nFirst, define a Beer-Lambert Model function that returns the vector of residuals between measured and modeled surface reflectance.\n# https://github.com/isofit/isofit/blob/main/isofit/inversion/inverse_simple.py#L514C1-L532C17\ndef beer_lambert_model(x, y, wl, alpha_lw):\n    \"\"\"Function, which computes the vector of residuals between measured and modeled surface reflectance optimizing\n    for path length of surface liquid water based on the Beer-Lambert attenuation law.\n\n    Args:\n        x:        state vector (liquid water path length, intercept, slope)\n        y:        measurement (surface reflectance spectrum)\n        wl:       instrument wavelengths\n        alpha_lw: wavelength dependent absorption coefficients of liquid water\n\n    Returns:\n        resid: residual between modeled and measured surface reflectance\n    \"\"\"\n\n    attenuation = np.exp(-x[0] * 1e7 * alpha_lw)\n    rho = (x[1] + x[2] * wl) * attenuation\n    resid = rho - y\n\n    return resid\nWe need some lab measurements of the complex refractive index of liquid water to obtain the wavelength-dependent absorption coefficients. They are calculated by taking four times the product of Pi and the imaginary part of the refractive index, divided by wavelength. The refractive index of liquid water per wavelength is provided by the k_liquid_water_ice.csv in the data folder. We can also preview this data to get a better understanding of the information we are using.\n\nwp_fp = '../data/k_liquid_water_ice.csv'\nk_wi = pd.read_csv(wp_fp)\nk_wi.head()\n\n\n\n\n\n  \n    \n      \n      wvl_1\n      T = 22°C\n      wvl_2\n      T = -8°C\n      wvl_3\n      T = -25°C\n      wvl_4\n      T = -7°C\n      wvl_5\n      T = 25°C (H)\n      wvl_6\n      T = 20°C\n      wvl_7\n      T = 25°C (S)\n      Index\n    \n  \n  \n    \n      0\n      666.7\n      2.470000e-08\n      NaN\n      NaN\n      NaN\n      NaN\n      660.0\n      1.660000e-08\n      650.0\n      1.640000e-08\n      650.0\n      1.870000e-08\n      650.12971\n      1.674130e-08\n      0\n    \n    \n      1\n      667.6\n      2.480000e-08\n      NaN\n      NaN\n      NaN\n      NaN\n      670.0\n      1.890000e-08\n      675.0\n      2.230000e-08\n      651.0\n      1.890000e-08\n      654.63616\n      1.777420e-08\n      1\n    \n    \n      2\n      668.4\n      2.480000e-08\n      NaN\n      NaN\n      NaN\n      NaN\n      680.0\n      2.090000e-08\n      700.0\n      3.350000e-08\n      652.0\n      1.910000e-08\n      660.69347\n      1.939950e-08\n      2\n    \n    \n      3\n      669.3\n      2.520000e-08\n      NaN\n      NaN\n      NaN\n      NaN\n      690.0\n      2.400000e-08\n      725.0\n      9.150000e-08\n      653.0\n      1.940000e-08\n      665.27314\n      2.031380e-08\n      3\n    \n    \n      4\n      670.2\n      2.530000e-08\n      NaN\n      NaN\n      NaN\n      NaN\n      700.0\n      2.900000e-08\n      750.0\n      1.560000e-07\n      654.0\n      1.970000e-08\n      669.88461\n      2.097930e-08\n      4\n    \n  \n\n\n\n\n\nfig, axs = plt.subplots(2,4, figsize=(15, 6),  sharex=True, sharey=True, constrained_layout=True)\naxs = axs.ravel()\ncol_n = 0\nfor i in range(0, 7):\n    x = k_wi.iloc[:, col_n+i]\n    y = k_wi.iloc[:, col_n+i+1]\n    axs[i].scatter(x, y)\n    axs[i].set_title(y.name)\n    col_n+=1\nfig.supylabel('imaginary parts of refractive index')\nfig.supxlabel('wavelength')\nplt.show()\n\n\n\n\nNow define a function to get the desired data from the csv file.\n# https://github.com/isofit/isofit/blob/dev/isofit/core/common.py#L461C1-L488C26\ndef get_refractive_index(k_wi, a, b, col_wvl, col_k):\n    \"\"\"Convert refractive index table entries to numpy array.\n\n    Args:\n        k_wi:    variable\n        a:       start line\n        b:       end line\n        col_wvl: wavelength column in pandas table\n        col_k:   k column in pandas table\n\n    Returns:\n        wvl_arr: array of wavelengths\n        k_arr:   array of imaginary parts of refractive index\n    \"\"\"\n\n    wvl_ = []\n    k_ = []\n\n    for ii in range(a, b):\n        wvl = k_wi.at[ii, col_wvl]\n        k = k_wi.at[ii, col_k]\n        wvl_.append(wvl)\n        k_.append(k)\n\n    wvl_arr = np.asarray(wvl_)\n    k_arr = np.asarray(k_)\n\n    return wvl_arr, k_arr\nLastly, to calculate CWC we define a function that uses least squares optimization to minimize the residuals of our Beer-Lambert Model and find a likely path length of liquid water.\n# https://github.com/isofit/isofit/blob/main/isofit/inversion/inverse_simple.py#L443C1-L511C24\ndef invert_liquid_water(\n    rfl_meas: np.array,\n    wl: np.array,\n    l_shoulder: float = 850,\n    r_shoulder: float = 1100,\n    lw_init: tuple = (0.02, 0.3, 0.0002),\n    lw_bounds: tuple = ([0, 0.5], [0, 1.0], [-0.0004, 0.0004]),\n    ewt_detection_limit: float = 0.5,\n    return_abs_co: bool = False,\n):\n    \"\"\"Given a reflectance estimate, fit a state vector including liquid water path length\n    based on a simple Beer-Lambert surface model.\n\n    Args:\n        rfl_meas:            surface reflectance spectrum\n        wl:                  instrument wavelengths, must be same size as rfl_meas\n        l_shoulder:          wavelength of left absorption feature shoulder\n        r_shoulder:          wavelength of right absorption feature shoulder\n        lw_init:             initial guess for liquid water path length, intercept, and slope\n        lw_bounds:           lower and upper bounds for liquid water path length, intercept, and slope\n        ewt_detection_limit: upper detection limit for ewt\n        return_abs_co:       if True, returns absorption coefficients of liquid water\n\n    Returns:\n        solution: estimated liquid water path length, intercept, and slope based on a given surface reflectance\n    \"\"\"\n    \n    # Ensure least squares is done with float64 datatype\n    wl = np.float64(wl)\n    \n    # params needed for liquid water fitting\n    lw_feature_left = np.argmin(abs(l_shoulder - wl))\n    lw_feature_right = np.argmin(abs(r_shoulder - wl))\n    wl_sel = wl[lw_feature_left : lw_feature_right + 1]\n\n    # adjust upper detection limit for ewt if specified\n    if ewt_detection_limit != 0.5:\n        lw_bounds[0][1] = ewt_detection_limit\n\n    # load imaginary part of liquid water refractive index and calculate wavelength dependent absorption coefficient\n    # __file__ should live at isofit/isofit/inversion/\n    \n    \n    data_dir_path = \"../data/\"\n    path_k = os.path.join(data_dir_path,\"k_liquid_water_ice.csv\")\n    \n    #isofit_path = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))\n    #path_k = os.path.join(isofit_path, \"data\", \"iop\", \"k_liquid_water_ice.xlsx\")\n\n    # k_wi = pd.read_excel(io=path_k, sheet_name=\"Sheet1\", engine=\"openpyxl\")\n    # wl_water, k_water = get_refractive_index(\n    #     k_wi=k_wi, a=0, b=982, col_wvl=\"wvl_6\", col_k=\"T = 20°C\"\n    # )\n    k_wi = pd.read_csv(path_k)\n    wl_water, k_water = get_refractive_index(\n        k_wi=k_wi, a=0, b=982, col_wvl=\"wvl_6\", col_k=\"T = 20°C\"\n    )\n    kw = np.interp(x=wl_sel, xp=wl_water, fp=k_water)\n    abs_co_w = 4 * np.pi * kw / wl_sel\n\n    rfl_meas_sel = rfl_meas[lw_feature_left : lw_feature_right + 1]\n\n    x_opt = least_squares(\n        fun=beer_lambert_model,\n        x0=lw_init,\n        jac=\"2-point\",\n        method=\"trf\",\n        bounds=(\n            np.array([lw_bounds[ii][0] for ii in range(3)]),\n            np.array([lw_bounds[ii][1] for ii in range(3)]),\n        ),\n        max_nfev=15,\n        args=(rfl_meas_sel, wl_sel, abs_co_w),\n    )\n\n    solution = x_opt.x\n\n    if return_abs_co:\n        return solution, abs_co_w\n    else:\n        return solution\n\n\nNow that we have all of the pieces, we can estimate the CWC of a single pixel using our invert_liquid_water function.\n\newt = invert_liquid_water(point.reflectance.values,point.wavelengths.values)\nprint(f\"EWT for ({point.longitude.values:.3f},{point.latitude.values:.3f}): {ewt[0]:.3f} cm\")\n\nEWT for (-120.353,34.540): 0.149 cm\n\n\n\n\n\nWe can also apply this function to the point data we selected in the previous notebook with the interactive plot.\nRead in the csv file we created.\n\npoints_df = pd.read_csv(\"../data/emit_click_data.csv\")\npoints_df\n\n\n\n\n\n  \n    \n      \n      id\n      x\n      y\n      381.00558\n      388.4092\n      395.81583\n      403.2254\n      410.638\n      418.0536\n      425.47214\n      ...\n      2426.4402\n      2433.8303\n      2441.2183\n      2448.6064\n      2455.9944\n      2463.3816\n      2470.7678\n      2478.153\n      2485.5386\n      2492.9238\n    \n  \n  \n    \n      0\n      0\n      -120.569492\n      34.574552\n      0.018037\n      0.017798\n      0.017567\n      0.017348\n      0.017523\n      0.018065\n      0.018957\n      ...\n      0.034822\n      0.034534\n      0.030492\n      0.031268\n      0.027023\n      0.026327\n      0.024637\n      0.023704\n      0.015793\n      0.013392\n    \n    \n      1\n      1\n      -120.568286\n      34.591054\n      0.012029\n      0.011957\n      0.011888\n      0.011826\n      0.012031\n      0.012486\n      0.013151\n      ...\n      0.020698\n      0.022204\n      0.020115\n      0.019911\n      0.016761\n      0.017437\n      0.015805\n      0.016146\n      0.014353\n      0.012923\n    \n    \n      2\n      2\n      -120.371752\n      34.484274\n      0.020833\n      0.021323\n      0.021819\n      0.022317\n      0.023065\n      0.024095\n      0.025361\n      ...\n      0.059581\n      0.058129\n      0.053620\n      0.053622\n      0.048783\n      0.044683\n      0.044064\n      0.041562\n      0.034080\n      0.028254\n    \n    \n      3\n      3\n      -120.181848\n      34.483303\n      0.024848\n      0.028178\n      0.034840\n      0.038940\n      0.043875\n      0.047232\n      0.050809\n      ...\n      0.131800\n      0.135400\n      0.122960\n      0.118844\n      0.108201\n      0.112605\n      0.106271\n      0.101222\n      0.070507\n      0.059903\n    \n    \n      4\n      4\n      -120.407924\n      34.591054\n      0.040804\n      0.044085\n      0.048493\n      0.054733\n      0.059349\n      0.064003\n      0.067749\n      ...\n      0.266081\n      0.263995\n      0.258724\n      0.252158\n      0.228920\n      0.226200\n      0.212124\n      0.181446\n      0.130708\n      0.111522\n    \n    \n      5\n      5\n      -120.438851\n      34.712298\n      0.015452\n      0.016034\n      0.016616\n      0.017204\n      0.017848\n      0.018621\n      0.019477\n      ...\n      0.046427\n      0.046706\n      0.043317\n      0.044288\n      0.038928\n      0.037683\n      0.038662\n      0.035936\n      0.034616\n      0.033542\n    \n    \n      6\n      6\n      -120.512401\n      34.648230\n      0.044019\n      0.039991\n      0.039511\n      0.045176\n      0.045753\n      0.047011\n      0.048643\n      ...\n      0.081432\n      0.080438\n      0.076009\n      0.076724\n      0.069166\n      0.069164\n      0.066233\n      0.063523\n      0.059582\n      0.056769\n    \n    \n      7\n      7\n      -120.350229\n      34.626389\n      0.018255\n      0.018679\n      0.019107\n      0.019550\n      0.020291\n      0.021369\n      0.022694\n      ...\n      0.050578\n      0.047565\n      0.046092\n      0.046638\n      0.039017\n      0.036911\n      0.037903\n      0.033778\n      0.028663\n      0.028218\n    \n    \n      8\n      8\n      -120.355052\n      34.713269\n      0.017943\n      0.018290\n      0.018641\n      0.019003\n      0.019586\n      0.020435\n      0.021492\n      ...\n      0.047776\n      0.048551\n      0.045117\n      0.045340\n      0.039024\n      0.038904\n      0.039077\n      0.035324\n      0.030073\n      0.029010\n    \n    \n      9\n      9\n      -120.243522\n      34.645318\n      0.021041\n      0.021242\n      0.021448\n      0.021661\n      0.022252\n      0.023214\n      0.024442\n      ...\n      0.049286\n      0.047242\n      0.042362\n      0.045277\n      0.039692\n      0.040241\n      0.037825\n      0.034632\n      0.029134\n      0.029248\n    \n  \n\n10 rows × 288 columns\n\n\n\nNow create an array of wavelengths from the column names starting with the first wavelength value (column 3).\n# Get wavelength values\nwavelength_values = points_df.columns[3::].to_numpy()\nIterate by row through our dataframe, selecting the reflectance values and providing them to the invert_liquid_water function. Afterwards, add an CWC column to our dataframe.\n# Creat empty list\newt_values = []\n# Iterate through rows\nfor _i in points_df.index.to_list():\n    # Get reflectance array to pass to function\n    rfl_values = points_df.iloc[_i,3::]\n    # Use invert liquid water function and append results to list\n    ewt_values.append(invert_liquid_water(rfl_values,wavelength_values)[0])    \n# Add to our existing dataframe at Column Index 3\npoints_df.insert(3, \"ewt\", ewt_values)\n\npoints_df\n\n\n\n\n\n  \n    \n      \n      id\n      x\n      y\n      ewt\n      381.00558\n      388.4092\n      395.81583\n      403.2254\n      410.638\n      418.0536\n      ...\n      2426.4402\n      2433.8303\n      2441.2183\n      2448.6064\n      2455.9944\n      2463.3816\n      2470.7678\n      2478.153\n      2485.5386\n      2492.9238\n    \n  \n  \n    \n      0\n      0\n      -120.569492\n      34.574552\n      2.773249e-01\n      0.018037\n      0.017798\n      0.017567\n      0.017348\n      0.017523\n      0.018065\n      ...\n      0.034822\n      0.034534\n      0.030492\n      0.031268\n      0.027023\n      0.026327\n      0.024637\n      0.023704\n      0.015793\n      0.013392\n    \n    \n      1\n      1\n      -120.568286\n      34.591054\n      2.495512e-01\n      0.012029\n      0.011957\n      0.011888\n      0.011826\n      0.012031\n      0.012486\n      ...\n      0.020698\n      0.022204\n      0.020115\n      0.019911\n      0.016761\n      0.017437\n      0.015805\n      0.016146\n      0.014353\n      0.012923\n    \n    \n      2\n      2\n      -120.371752\n      34.484274\n      1.639785e-01\n      0.020833\n      0.021323\n      0.021819\n      0.022317\n      0.023065\n      0.024095\n      ...\n      0.059581\n      0.058129\n      0.053620\n      0.053622\n      0.048783\n      0.044683\n      0.044064\n      0.041562\n      0.034080\n      0.028254\n    \n    \n      3\n      3\n      -120.181848\n      34.483303\n      5.938623e-02\n      0.024848\n      0.028178\n      0.034840\n      0.038940\n      0.043875\n      0.047232\n      ...\n      0.131800\n      0.135400\n      0.122960\n      0.118844\n      0.108201\n      0.112605\n      0.106271\n      0.101222\n      0.070507\n      0.059903\n    \n    \n      4\n      4\n      -120.407924\n      34.591054\n      4.001294e-11\n      0.040804\n      0.044085\n      0.048493\n      0.054733\n      0.059349\n      0.064003\n      ...\n      0.266081\n      0.263995\n      0.258724\n      0.252158\n      0.228920\n      0.226200\n      0.212124\n      0.181446\n      0.130708\n      0.111522\n    \n    \n      5\n      5\n      -120.438851\n      34.712298\n      6.409762e-02\n      0.015452\n      0.016034\n      0.016616\n      0.017204\n      0.017848\n      0.018621\n      ...\n      0.046427\n      0.046706\n      0.043317\n      0.044288\n      0.038928\n      0.037683\n      0.038662\n      0.035936\n      0.034616\n      0.033542\n    \n    \n      6\n      6\n      -120.512401\n      34.648230\n      2.607293e-10\n      0.044019\n      0.039991\n      0.039511\n      0.045176\n      0.045753\n      0.047011\n      ...\n      0.081432\n      0.080438\n      0.076009\n      0.076724\n      0.069166\n      0.069164\n      0.066233\n      0.063523\n      0.059582\n      0.056769\n    \n    \n      7\n      7\n      -120.350229\n      34.626389\n      1.505599e-01\n      0.018255\n      0.018679\n      0.019107\n      0.019550\n      0.020291\n      0.021369\n      ...\n      0.050578\n      0.047565\n      0.046092\n      0.046638\n      0.039017\n      0.036911\n      0.037903\n      0.033778\n      0.028663\n      0.028218\n    \n    \n      8\n      8\n      -120.355052\n      34.713269\n      1.379378e-01\n      0.017943\n      0.018290\n      0.018641\n      0.019003\n      0.019586\n      0.020435\n      ...\n      0.047776\n      0.048551\n      0.045117\n      0.045340\n      0.039024\n      0.038904\n      0.039077\n      0.035324\n      0.030073\n      0.029010\n    \n    \n      9\n      9\n      -120.243522\n      34.645318\n      1.954748e-01\n      0.021041\n      0.021242\n      0.021448\n      0.021661\n      0.022252\n      0.023214\n      ...\n      0.049286\n      0.047242\n      0.042362\n      0.045277\n      0.039692\n      0.040241\n      0.037825\n      0.034632\n      0.029134\n      0.029248\n    \n  \n\n10 rows × 289 columns\n\n\n\nFor larger sets of point data we would want to do this in parallel. We can also calculate CWC for an area, where we definitly need to utilize parallel processing.\n\n\n\n\nIn the previous notebook, we subset our region of interest and exported the file. Since the CWC calculation is computationally intensive, it can take a while to process large scenes, so its more efficient to do this spatial subsetting up front. We can use a function included in the ewt_calc.py module to calculate CWC on a cropped image, and create a cloud-optimized GeoTIFF (COG) file containing the results.\nSet our input filepaths and output directory.\nfp = '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/rfl/EMIT_L2A_RFL_001_20230401T203751_2309114_002_dangermond.nc'\nout_dir = '../data/'\n\nroi_ds = xr.open_dataset('../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/rfl/EMIT_L2A_RFL_001_20230401T203751_2309114_002_dangermond.nc', decode_coords='all')\nroi_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:           (wavelengths: 285, latitude: 244, longitude: 262)\nCoordinates:\n  * wavelengths       (wavelengths) float32 381.0 388.4 ... 2.486e+03 2.493e+03\n    fwhm              (wavelengths) float32 ...\n    good_wavelengths  (wavelengths) float32 ...\n  * latitude          (latitude) float64 34.57 34.57 34.57 ... 34.44 34.44 34.44\n  * longitude         (longitude) float64 -120.5 -120.5 -120.5 ... -120.4 -120.4\n    elev              (latitude, longitude) float32 ...\n    spatial_ref       int64 ...\nData variables:\n    reflectance       (latitude, longitude, wavelengths) float32 ...\nAttributes: (12/39)\n    ncei_template_version:             NCEI_NetCDF_Swath_Template_v2.0\n    summary:                           The Earth Surface Mineral Dust Source ...\n    keywords:                          Imaging Spectroscopy, minerals, EMIT, ...\n    Conventions:                       CF-1.63\n    sensor:                            EMIT (Earth Surface Mineral Dust Sourc...\n    instrument:                        EMIT\n    ...                                ...\n    spatialResolution:                 0.000542232520256367\n    spatial_ref:                       GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHER...\n    geotransform:                      [-1.20992414e+02  5.42232520e-04 -0.00...\n    day_night_flag:                    Day\n    title:                             EMIT L2A Estimated Surface Reflectance...\n    granule_id:                        EMIT_L2A_RFL_001_20230401T203751_23091...xarray.DatasetDimensions:wavelengths: 285latitude: 244longitude: 262Coordinates: (7)wavelengths(wavelengths)float32381.0 388.4 ... 2.486e+03 2.493e+03long_name :Wavelength Centersunits :nmarray([ 381.00558,  388.4092 ,  395.81583, ..., 2478.153  , 2485.5386 ,\n       2492.9238 ], dtype=float32)fwhm(wavelengths)float32...long_name :Full Width at Half Maxunits :nm[285 values with dtype=float32]good_wavelengths(wavelengths)float32...long_name :Wavelengths where reflectance is useable: 1 = good data, 0 = bad dataunits :unitless[285 values with dtype=float32]latitude(latitude)float6434.57 34.57 34.57 ... 34.44 34.44long_name :latitudeunits :degrees_northaxis :Ystandard_name :latitudearray([34.57432 , 34.573777, 34.573235, ..., 34.443642, 34.443099, 34.442557])longitude(longitude)float64-120.5 -120.5 ... -120.4 -120.4long_name :longitudeunits :degrees_eastaxis :Xstandard_name :longitudearray([-120.499254, -120.498711, -120.498169, ..., -120.358815, -120.358273,\n       -120.357731])elev(latitude, longitude)float32...long_name :Surface Elevationunits :m[63928 values with dtype=float32]spatial_ref()int64...crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-120.49952471405297 0.0005422325202563338 0.0 34.57459066750936 0.0 -0.0005422325202563691[1 values with dtype=int64]Data variables: (1)reflectance(latitude, longitude, wavelengths)float32...long_name :Surface Reflectanceunits :unitless[18219480 values with dtype=float32]Indexes: (3)wavelengthsPandasIndexPandasIndex(Float64Index([ 381.0055847167969,  388.4092102050781,  395.8158264160156,\n              403.22540283203125, 410.63800048828125,  418.0535888671875,\n               425.4721374511719,  432.8927001953125,  440.3172607421875,\n               447.7427978515625,\n              ...\n               2426.440185546875,  2433.830322265625,   2441.21826171875,\n                 2448.6064453125,  2455.994384765625,  2463.381591796875,\n               2470.767822265625,  2478.153076171875,   2485.53857421875,\n                  2492.923828125],\n             dtype='float64', name='wavelengths', length=285))latitudePandasIndexPandasIndex(Float64Index([ 34.57431955124923,  34.57377731872898,  34.57323508620872,\n              34.572692853688466, 34.572150621168205,  34.57160838864795,\n                34.5710661561277,  34.57052392360744, 34.569981691087186,\n              34.569439458566926,\n              ...\n               34.44743714150925,  34.44689490898899, 34.446352676468734,\n              34.445810443948474,  34.44526821142822,  34.44472597890796,\n               34.44418374638771,  34.44364151386745, 34.443099281347195,\n              34.442557048826934],\n             dtype='float64', name='latitude', length=244))longitudePandasIndexPandasIndex(Float64Index([-120.49925359779284, -120.49871136527258, -120.49816913275234,\n              -120.49762690023208, -120.49708466771182, -120.49654243519157,\n              -120.49600020267131, -120.49545797015105, -120.49491573763079,\n              -120.49437350511054,\n              ...\n              -120.36261100268824, -120.36206877016798, -120.36152653764773,\n              -120.36098430512747, -120.36044207260721, -120.35989984008695,\n              -120.35935760756671, -120.35881537504645, -120.35827314252619,\n              -120.35773091000594],\n             dtype='float64', name='longitude', length=262))Attributes: (39)ncei_template_version :NCEI_NetCDF_Swath_Template_v2.0summary :The Earth Surface Mineral Dust Source Investigation (EMIT) is an Earth Ventures-Instrument (EVI-4) Mission that maps the surface mineralogy of arid dust source regions via imaging spectroscopy in the visible and short-wave infrared (VSWIR). Installed on the International Space Station (ISS), the EMIT instrument is a Dyson imaging spectrometer that uses contiguous spectroscopic measurements from 410 to 2450 nm to resolve absoprtion features of iron oxides, clays, sulfates, carbonates, and other dust-forming minerals. During its one-year mission, EMIT will observe the sunlit Earth's dust source regions that occur within +/-52° latitude and produce maps of the source regions that can be used to improve forecasts of the role of mineral dust in the radiative forcing (warming or cooling) of the atmosphere.\\n\\nThis file contains L2A estimated surface reflectances and geolocation data. Reflectance estimates are created using an Optimal Estimation technique - see ATBD for details. Reflectance values are reported as fractions (relative to 1). Geolocation data (latitude, longitude, height) and a lookup table to project the data are also included.keywords :Imaging Spectroscopy, minerals, EMIT, dust, radiative forcingConventions :CF-1.63sensor :EMIT (Earth Surface Mineral Dust Source Investigation)instrument :EMITplatform :ISSinstitution :NASA Jet Propulsion Laboratory/California Institute of Technologylicense :https://science.nasa.gov/earth-science/earth-science-data/data-information-policy/naming_authority :LPDAACdate_created :2023-04-06T02:26:24Zkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstdname_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventioncreator_name :Jet Propulsion Laboratory/California Institute of Technologycreator_url :https://earth.jpl.nasa.gov/emit/project :Earth Surface Mineral Dust Source Investigationproject_url :https://emit.jpl.nasa.gov/publisher_name :NASA LPDAACpublisher_url :https://lpdaac.usgs.govpublisher_email :lpdaac@usgs.govidentifier_product_doi_authority :https://doi.orgflight_line :emit20230401t203751_o09114_s000time_coverage_start :2023-04-01T20:37:51+0000time_coverage_end :2023-04-01T20:38:03+0000software_build_version :010610software_delivery_version :010610product_version :V001history :PGE Run Command: {python /beegfs/store/emit/ops/repos/emit-sds-l2a/spectrum_quality.py /tmp/emit/ops/emit20230401t203751_emit.L2AReflectance_20230405t233102/output/emit20230401t203751_rfl /tmp/emit/ops/emit20230401t203751_emit.L2AReflectance_20230405t233102/output/emit20230401t203751_rfl_quality.txt}, PGE Input Files: {radiance_file=/beegfs/store/emit/ops/data/acquisitions/20230401/emit20230401t203751/l1b/emit20230401t203751_o09114_s000_l1b_rdn_b0106_v01.img, pixel_locations_file=/beegfs/store/emit/ops/data/acquisitions/20230401/emit20230401t203751/l1b/emit20230401t203751_o09114_s000_l1b_loc_b0106_v01.img, observation_parameters_file=/beegfs/store/emit/ops/data/acquisitions/20230401/emit20230401t203751/l1b/emit20230401t203751_o09114_s000_l1b_obs_b0106_v01.img, surface_model_config=/beegfs/store/emit/ops/repos/emit-sds-l2a/surface/surface_20221020.json}crosstrack_orientation :as seen on groundeasternmost_longitude :-119.71545648976226northernmost_latitude :34.7990749308955westernmost_longitude :-120.992414074966southernmost_latitude :33.77100207248943spatialResolution :0.000542232520256367spatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]geotransform :[-1.20992414e+02  5.42232520e-04 -0.00000000e+00  3.47990749e+01\n -0.00000000e+00 -5.42232520e-04]day_night_flag :Daytitle :EMIT L2A Estimated Surface Reflectance 60 m V001granule_id :EMIT_L2A_RFL_001_20230401T203751_2309114_002\n\n\n\nroi_ds.sel(wavelengths=850,method='nearest').hvplot.image(x='longitude',y='latitude',cmap='viridis',geo=True, tiles='ESRI', frame_width=720,frame_height=405, alpha=0.7, fontscale=2).opts(\n    title=f\"Dangermond ROI - RFL at 850 nm\", xlabel='Longitude',ylabel='Latitude')\n\nUnable to display output for mime type(s): \n\n\n\n  \n\n\n\n\nUse the calc_ewt function to calculate CWC of the cropped image. This function will also create a COG file containing the CWC results. We can also specify the number of CPUs to use manually with a n_cpu argument, or leave it blank to use all but one of the available CPUs. If we set the return_cwc argument to true, the function will also return the COG.\nThis will take some time, about 5 minutes, because we’re doing the calculation for roughly 63,000 pixels.\n\n%%time\nds_cwc = calc_ewt(fp, out_dir, return_cwc=True)\n\n2023-12-07 15:11:32,965 WARNING services.py:1996 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.05gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n2023-12-07 15:11:34,128 INFO worker.py:1664 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265 \n\n\nCPU times: user 688 ms, sys: 315 ms, total: 1 s\nWall time: 5min\n\n\n\nds_cwc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset>\nDimensions:      (latitude: 244, longitude: 262)\nCoordinates:\n  * latitude     (latitude) float64 34.57 34.57 34.57 ... 34.44 34.44 34.44\n  * longitude    (longitude) float64 -120.5 -120.5 -120.5 ... -120.4 -120.4\n    elev         (latitude, longitude) float32 ...\n    spatial_ref  int64 ...\nData variables:\n    cwc          (latitude, longitude) float64 nan nan nan nan ... nan nan nan\nAttributes: (12/13)\n    flight_line:            emit20230401t203751_o09114_s000\n    time_coverage_start:    2023-04-01T20:37:51+0000\n    time_coverage_end:      2023-04-01T20:38:03+0000\n    easternmost_longitude:  -119.71545648976226\n    northernmost_latitude:  34.7990749308955\n    westernmost_longitude:  -120.992414074966\n    ...                     ...\n    spatialResolution:      0.000542232520256367\n    spatial_ref:            GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84...\n    geotransform:           [-1.20992414e+02  5.42232520e-04 -0.00000000e+00 ...\n    day_night_flag:         Day\n    title:                  EMIT Estimated Equivalent Water Thickness (EWT) /...\n    granule_id:             EMIT_L2A_RFL_001_20230401T203751_2309114_002xarray.DatasetDimensions:latitude: 244longitude: 262Coordinates: (4)latitude(latitude)float6434.57 34.57 34.57 ... 34.44 34.44long_name :latitudeunits :degrees_northaxis :Ystandard_name :latitudearray([34.57432 , 34.573777, 34.573235, ..., 34.443642, 34.443099, 34.442557])longitude(longitude)float64-120.5 -120.5 ... -120.4 -120.4long_name :longitudeunits :degrees_eastaxis :Xstandard_name :longitudearray([-120.499254, -120.498711, -120.498169, ..., -120.358815, -120.358273,\n       -120.357731])elev(latitude, longitude)float32...long_name :Surface Elevationunits :m[63928 values with dtype=float32]spatial_ref()int64...crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-120.49952471405297 0.0005422325202563338 0.0 34.57459066750936 0.0 -0.0005422325202563691[1 values with dtype=int64]Data variables: (1)cwc(latitude, longitude)float64nan nan nan nan ... nan nan nan nanlong_name :Canopy Water Contentunits :g/cm^2_FillValue :-9999array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])Indexes: (2)latitudePandasIndexPandasIndex(Float64Index([ 34.57431955124923,  34.57377731872898,  34.57323508620872,\n              34.572692853688466, 34.572150621168205,  34.57160838864795,\n                34.5710661561277,  34.57052392360744, 34.569981691087186,\n              34.569439458566926,\n              ...\n               34.44743714150925,  34.44689490898899, 34.446352676468734,\n              34.445810443948474,  34.44526821142822,  34.44472597890796,\n               34.44418374638771,  34.44364151386745, 34.443099281347195,\n              34.442557048826934],\n             dtype='float64', name='latitude', length=244))longitudePandasIndexPandasIndex(Float64Index([-120.49925359779284, -120.49871136527258, -120.49816913275234,\n              -120.49762690023208, -120.49708466771182, -120.49654243519157,\n              -120.49600020267131, -120.49545797015105, -120.49491573763079,\n              -120.49437350511054,\n              ...\n              -120.36261100268824, -120.36206877016798, -120.36152653764773,\n              -120.36098430512747, -120.36044207260721, -120.35989984008695,\n              -120.35935760756671, -120.35881537504645, -120.35827314252619,\n              -120.35773091000594],\n             dtype='float64', name='longitude', length=262))Attributes: (13)flight_line :emit20230401t203751_o09114_s000time_coverage_start :2023-04-01T20:37:51+0000time_coverage_end :2023-04-01T20:38:03+0000easternmost_longitude :-119.71545648976226northernmost_latitude :34.7990749308955westernmost_longitude :-120.992414074966southernmost_latitude :33.77100207248943spatialResolution :0.000542232520256367spatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]geotransform :[-1.20992414e+02  5.42232520e-04 -0.00000000e+00  3.47990749e+01\n -0.00000000e+00 -5.42232520e-04]day_night_flag :Daytitle :EMIT Estimated Equivalent Water Thickness (EWT) / Canopy Water Content (CWC)granule_id :EMIT_L2A_RFL_001_20230401T203751_2309114_002\n\n\nPlot CWC of our ROI.\n\nds_cwc.hvplot.image(x='longitude',y='latitude',cmap='viridis',geo=True, tiles='ESRI', frame_width=720,frame_height=405, alpha=0.7, fontscale=2).opts(\n    title=f\"{ds_cwc.cwc.long_name} ({ds_cwc.cwc.units})\", xlabel='Longitude',ylabel='Latitude')\n\nUnable to display output for mime type(s): \n\n\n\n  \n\n\n\n\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 11-28-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "python/04_Dangermond_Land_Cover.html",
    "href": "python/04_Dangermond_Land_Cover.html",
    "title": "VITALS",
    "section": "",
    "text": "Authors\nChristiana Ade1 and Marie Johnson1,2 1. Jet Propulsion Laboratory, California Institute of Technology\n2. University of Montana\nSummary\nIn this notebook, we will examine how Canopy Water Content (CWC) derived from EMIT data and Land Surface Temperature (LST) derived from ECOSTRESS change across three different vegetation types and dates in the Jack and Laura Dangermond preserve.\n\nNote in previous notebooks, we referenced the CWC product as Equivalent Water Thickness (EWT) because water signal can still be retrieved from non-vegetated surfaces. But, here we will be investigating vegetated surfaces and will reference it as CWC.*\n\nLearning Objectives\n- Use product time series related to a real life scenario concerning invasive species in nature preserves.\nBackground\nIn 2017, The Nature Conservancy (TNC) acquired the preserve through a generous donation from Jack and Laura Dangermond. The preserve is 24,460 acres of a former private ranch situated at Point Conception, California. Point conception represents the boundary between Northern and Southern California, which separates terrestrial, marine, and coastal ecoregions. This is one of only a few areas globally where the boundary between marine and terrestrial ecosystems exist together. The preserve is one of the last “wild coast” areas in Southern California and it has some of the highest biodiversity globally. The Dangermond Preserve offers a unique opportunity to study the impacts of global change; sea level rise, the intensification of wildfire and drought providing a natural laboratory.\nThe preserve is naturally composed of oak tree forests and native grasslands; however, in recent years it faces several invasion issues from non-native grasses and iceplant (Carpobrotus spp.). The currently mapped invasive plant species at the Preserve include non-native grasslands, iceplant mats, and stands of black mustard (Brassica nigra), poison hemlock (Conium maculatum), thistles, and fennel (Foeniculum vulgare). Iceplant is one of the priority species listed under the preserve’s invasive species management plan.\n\n  \n\nFigure 1. Iceplants along California’s coastline.\n\n\nIceplant is an invasive species native to South Africa that was introduced to California in the 1500s. Because of its ability to stabilize soil in the 1950’s, the Department of Transportation used iceplant to prevent erosion on roadsides. This practice lasted for 20 years before it was discontinued, however, many homeowners still use iceplant for landscaping today. Although iceplant can help prevent coastal erosion, it threatens coastal biodiversity. Iceplant takes up a large amount of soil moisture, thereby taking moisture away from other species. It can also inhibit the establishment of native grass species as a result of a high nitrate accumulation in iceplant soils. Current management efforts to eradicate iceplant at the Dangermond Preserve include cattle grazing. TNC is interested in utilizing enhanced remote sensing techniques to understand iceplant characteristics and detect iceplant locations.\n\n\n\nFigure 2. Invasive species locations at the perserve in 2021.\n\n\nExercise information\nThe cropped EMIT and ECOSTRESS imagery used in this notebook were created using code from previous tutorial notebooks. We have simply cropped this imagery to the Dangermond boundaries and saved them to the openscapes. As a reference, the tutorial code needed crop EMIT and ECOSTRESS products is in 02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.ipynb in sections 2.2.3 Cropping EMIT data to a Region of Interest, 2.2.4 Write an output, 2.3.2 Cropping ECOSTRESS Data, and 2.3.3 Writing Outputs.\nTutorial references\n01_Finding_Concurrent_Data.ipynb - In this notebook we learned how to use earthacces to find concurrent EMIT and ECOSTRESS data - We also learned how to export a list of files and download them programatically.\n02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.ipynb - How to open and work with EMIT L2A Reflectance and ECOSTRESS L2T LSTE data - How to apply a quality mask to EMIT datasets - How to reproject and regrid data - How to crop EMIT and ECOSTRESS data\n03_EMIT_CWC_from_Reflectance.ipynb - Calculate the EWT of a single pixel - Calculate the EWT of a ROI\nRequired datasets\n1. Image datasets: This imagery has already been processed and uploaded to openscapes using code from previous notebooks. We will use three dates in 2023 to represent different seasons: April 1st, June 29th, and September 23rd.\n\nEMIT Canopy Water Content\n\nEMIT_L2A_RFL_001_20230401T203751_2309114_002_dangermond_cwc.tif\nEMIT_L2A_RFL_001_20230629T170449_2318011_002_dangermond_cwc.tif\nEMIT_L2A_RFL_001_20230923T232101_2326615_002_dangermond_cwc.tif\n\nECOSTRESS Land Surface Temperature\n\nECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST_dangermond.tif\nECOv002_L2T_LSTE_28238_012_10SGD_20230629T170416_0710_01_LST_dangermond.tif\nECOv002_L2T_LSTE_29576_005_10SGD_20230923T232104_0710_01_LST_dangermond.tif\n\n\n\nVector datasets:\n\nveg_plots_outline.geojson : This file includes polygons of delinated vegetation plots of iceplant, non-native grassland, and coastal live oak polygons. This file was created specifically for this tutorial and the polygons were delinated using vegtation maps of Dangermond and field data collected during the SHIFT campaign.\n\n\nTutorial Outline\n4.1 View the CWC and LST images along with the vegetation class polygons\n4.2 Extract the CWC Raster Values\n4.3 Make a boxplot of CWC Extracted Values\n4.4 Extract and create box plot of LST images\n4.5 Investigate and discuss the differences between vegetation classes and dates for both CWC and LST\n4.6 Interactive playground! - Create your own polygons for extraction\nReferences * Bossard, C. C., Randall, J. M., & Hoshovsky, M. C. (Eds.). (2000). Invasive Plants of California’s Wildlands. University of California Press. * Butterfield, H.S., M. Reynolds, M.G. Gleason, M. Merrifield, B.S. Cohen, W.N. Heady, D. Cameron, T. Rick, E. Inlander, M. Katkowski, L. Riege, J. Knapp, S. Gennet, G. Gorga, K. Lin, K. Easterday, B. Leahy and M. Bell. 2019. Jack and Laura Dangermond Preserve Integrated Resources Management Plan. The Nature Conservancy. 112 pages.\n* Iceplant images https://centralcoastparks.org/ice-plant-the-iconic-but-destructive-piece-of-california-coastal-landscape/ https://sanctuaries.noaa.gov/news/feb15/invasive-species.html\n\n\n\n# Import required packages \nimport os\nimport glob\nimport math\nimport earthaccess\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray as rxr\nfrom rioxarray.merge import merge_arrays\nfrom matplotlib import pyplot as plt\nimport hvplot.xarray\nimport hvplot.pandas\nimport holoviews as hv\nimport geoviews as gv\nimport geopandas as gp\nimport sys\nfrom modules.emit_tools import emit_xarray, ortho_xr\nimport re\nimport warnings\nimport panel as pn\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom shapely.geometry import Polygon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s make a list called cwc_list of all the previously cropped EMIT CWC images by searching for tifs in our image directory cropped/dangermond/ewt/.\n\n# data directory - location of all images cropped to dangermond. Includes subfolders for ewt and lst. \ndata_dir = \"../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond\"\n# list all files in the ewt sub-directory of the data_dir path that end in tif\ncwc_list = glob.glob(os.path.join(data_dir, \"ewt\", \"*.tif\"))\ncwc_list\n\n['../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230401T203751_2309114_002_dangermond_cwc.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230219T202939_2305013_002_dangermond_cwc.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230422T195924_2311213_002_dangermond_cwc.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230405T190311_2309513_002_dangermond_cwc.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20231014T224006_2328715_002_dangermond_cwc.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230923T232101_2326615_002_dangermond_cwc.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230219T202951_2305013_003_dangermond_cwc.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230219T202939_2305013_002_dangermond_cwc_merged.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230629T170449_2318011_002_dangermond_cwc.tif']\n\n\ncwc_list is a list of nine previously processed EMIT scenes over Dangermond. However, several of those scenes do not capture the full preserve. In these first parts of the notebook, we fill focus on three dates by making a filtered list of the cwc files calledfil_cwc_list. We will use these dates to subset - Spring: 2023-04-01 - Summer: 2023-06-29 - Fall: 2023-09-23\n\n# the date and time codes strings we want to select from our larger cwc_list\ncwc_dates = ['20230401T203751', '20230629T170449', '20230923T232101']\n# this is written as a loop so that the files appear dates would appear chronologically like our cwc_date list\nfil_cwc_list = [] # empty list\nfor date in cwc_dates:\n    # filter for appropriate dates and then add back\n    fil_cwc_list.extend([file for file in cwc_list if date in file])\nfil_cwc_list\n\n['../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230401T203751_2309114_002_dangermond_cwc.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230629T170449_2318011_002_dangermond_cwc.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230923T232101_2326615_002_dangermond_cwc.tif']\n\n\n\n\nNext we will write a loop to visualize the change in CWC through time.\n\n# Initialize an empty list to store the plots\nplots = []\n\n# Iterate over each CWC file and create a plot\nfor file in fil_cwc_list:\n    # Open the raster file\n    ras = rxr.open_rasterio(file).squeeze('band', drop=True)\n    \n    # extract the date string from the file name using string matching\n    ras_date =  re.search(r\"\\d{8}\", file).group()\n    date_object = datetime.strptime(ras_date, \"%Y%m%d\")\n    date_only = date_object.date()#.strftime(\"%Y-%m-%d\")\n    \n    # Create a plot using geoviews\n    plot = ras.hvplot.image(x='x', y='y', geo=True, cmap='blues',\n                                                    tiles='ESRI', \n                                                    title=f\"{date_only} {ras.long_name} ({ras.units})\",\n                                                    xlabel='Longitude', ylabel='Latitude',\n                                                    frame_width=400, frame_height= 300,\n                                                    fontscale=1, alpha=0.7)\n    \n    # Add the plot to the list\n    plots.append(plot)\n\n# Display all plots in a grid layout using Panel\ngrid = pn.GridSpec(sizing_mode='stretch_both')\nfor i, plot in enumerate(plots):\n    grid[i // 2, i % 2] = plot  # Adjust 3 to change the number of columns\n\ngrid.servable()\n\nUnable to display output for mime type(s): \n\n\nUnable to display output for mime type(s): \n\n\n\n\n  \n\n\n\n\n\nThe south west corner of Dangermond seems to have a much higher CWC than other regions. From Figure 2, we can see that this is likely an very large iceplant patch. What other patterns do you see?\n\n\n\n\nHere we will load previously delineated vegetation plots of iceplant, coastal live oak and non native grasslands from the veg_plots_outline.geojson as the variable veg_poly. These plots come from a combination of field data and previous maps created for Dangermond preserve management.\n\n\n\nFigure 3. Examples of the different vegetation types in the veg_plots_outline.geojson .\n\n\n# read in veg polygons\nveg_poly = gp.read_file(\"../data/veg_plots_outline.geojson\")\n# let's load in one image from september for the plot visualization \ncw3 = rxr.open_rasterio(fil_cwc_list[2]).squeeze('band',drop=True)\nLet’s visualize this polygon dataset on top of the September CWC scene. On the left side, plot_1 shows the vegetation plots in red overlayed on the CWC scene. On the right side plot_2 has the polygons colored by vegetation class.\nImportant note for plot_1 code, in order to get the vegetation polygons to overlay we needed to use the star expression * and convert the veg_poly crs to EPSG:3857, which is the projection commonly used for web mapping. Even though cw3 and veg_poly originally had the same CRS, the holoviews package reprojects cw3 on the fly to EPSG:3857 for web mapping when we used hvplot.image. Therefore, we have to make the same transformation using veg_poly.to_crs('EPSG:3857')\n\nwarnings.filterwarnings('ignore') # We are supressing warnings for this cell since the package is just warning us that the function will change in a future package version.\n\n# plot one reference \nplot_1 = cw3.hvplot.image(x='x',y='y',geo=True, cmap='blues',tiles='ESRI',\n                          title=f\"{cw3.long_name} ({cw3.units})\",\n                          xlabel='Longitude',ylabel='Latitude', frame_width=300,\n                          frame_height=300, fontscale=1, alpha = 0.7) * veg_poly.to_crs('EPSG:3857').hvplot(c='red',alpha=1)\n \nplot_2 = veg_poly.hvplot.polygons(geo=True, c='Class', hover_cols='all', \n                                         xlabel='Longitude',ylabel='Latitude', frame_width=300,\n                                         frame_height=300, fontscale=1, title='Dangermond Vegetation Plots')\n\nplot_1 + plot_2\n\nUnable to display output for mime type(s): \n\n\n\n  \n\n\n\n\nTo zoom in click the box or wheel zoom icon.\n\n\n\nIn this section, we will use a previously written extract function called extract_raster_values from the accompanying script jlpd_ras_funcs.py. This script has all complex functions written for this notebook which is why we are using import *.. The other fucntions will be referenced later in the interactive portion of the notebook.\n# This is one way to load functions written from another script\nfrom modules.jldp_ras_funcs import *\nHere we will create a loop to extract the raster values from every pixel in each polygon of the veg_plots_outline.shp file for the spring, summer, and fall cwc_dates. The final output dataframe file will be called subset_df.\n# Create an empty list to store extracted values in\nex_df = [] \n\n# loop through each raster in fil_cwc_list\nfor r in fil_cwc_list:\n    # open the raster and set NA value to -9999\n    ras = rxr.open_rasterio(r).squeeze('band', drop=True) \n    ras.data[ras.data == -9999] = np.nan \n    # Extract raster values\n    ex_ras = extract_raster_values(ras, veg_poly) \n    \n    # Extract the iamge date from the raster itself and format correctly \n    date_object = datetime.strptime(ras.attrs['time_coverage_end'], \"%Y-%m-%dT%H:%M:%S%z\") \n    date_only = date_object.date()\n    # Add date to new column in data frame\n    ex_ras['rasDate'] = date_only \n    \n    # Append to list\n    ex_df.append(ex_ras) \n\n# concat all list entries into a dataframe\nex_cwc = pd.concat(ex_df).reset_index(drop=True)\n # Rename column value to CWC\nex_cwc.rename(columns={'value': 'CWC'}, inplace=True)\n# Replace -9999 with NA in the CWC column\nex_cwc['CWC'].replace(-9999, np.nan, inplace=True) \n# Remove rows with NA values in the CWC column\nex_cwc = ex_cwc.dropna(subset=['CWC']) \n\n# Preview data\nex_cwc.head()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      spatial_ref\n      CWC\n      cell_number\n      id\n      Class\n      geometry\n      rasDate\n    \n  \n  \n    \n      0\n      -120.461840\n      34.452859\n      0\n      0.5\n      34.45285946671181_-120.46183955389516\n      0\n      ice_plant\n      MULTIPOLYGON (((-120.46214771722677 34.4533973...\n      2023-04-01\n    \n    \n      1\n      -120.461840\n      34.452317\n      0\n      0.5\n      34.45231723419155_-120.46183955389516\n      0\n      ice_plant\n      MULTIPOLYGON (((-120.46214771722677 34.4533973...\n      2023-04-01\n    \n    \n      2\n      -120.461840\n      34.451775\n      0\n      0.5\n      34.451775001671294_-120.46183955389516\n      0\n      ice_plant\n      MULTIPOLYGON (((-120.46214771722677 34.4533973...\n      2023-04-01\n    \n    \n      3\n      -120.461840\n      34.451233\n      0\n      0.5\n      34.451232769151034_-120.46183955389516\n      0\n      ice_plant\n      MULTIPOLYGON (((-120.46214771722677 34.4533973...\n      2023-04-01\n    \n    \n      4\n      -120.461297\n      34.452859\n      0\n      0.5\n      34.45285946671181_-120.4612973213749\n      0\n      ice_plant\n      MULTIPOLYGON (((-120.46214771722677 34.4533973...\n      2023-04-01\n    \n  \n\n\n\n\nLet’s make a quick boxplot of our extracted values across seasons just to check that our extract function worked.\n\nfig, ax = plt.subplots(figsize=(7,3))\n# add boxplot \nb = sns.boxplot(x = 'rasDate', \n                y = 'CWC', \n                data = ex_cwc,\n                ax = ax,\n               width = 0.3,\n               palette = {\"#56B4E9\", \"#009E73\", \"#CC79A7\"}) # Create color palette\nb.set_ylabel(\"Canopy Water Content g/cm$\\mathregular{^2}$\", fontsize = 12)\nb.set_xlabel(\"Date\", fontsize = 12)\nb.set_title(\"Seasonal Changes in Canopy Water Content\", fontsize = 16)\n\n# overlay points for all values\nb = sns.stripplot(data = ex_cwc,\n                       x = 'rasDate', \n                       y = 'CWC', \n                       ax = ax,\n                      color = \"black\", # Colours the dots\n                      linewidth = 1,     # Dot outline width\n                      alpha = 0.4)       # Makes them transparent\n\n\n\n\n\n\n\n\nGreat! We manged to extract all the pixel values, but we are more interested in investigating the differences across type and vegetation plot type. Here, we separate the above plot into three faceted plots, one for each vegetation type. The final plot is called p_class.\n\nwarnings.filterwarnings('ignore') # We are supressing warnings for this cell since the package is just warning us that the function will change in a future package version.\n\np_class = sns.catplot(\n    x='rasDate', \n    y='CWC', \n    col='Class',  # This will create a separate plot for each unique value in the 'Class' column\n    data=ex_cwc, \n    kind='box',\n    col_wrap=3,  # Adjust this depending on how many plots per row you want\n    sharex=False, \n    sharey=False,\n    height=5, \n    aspect=1,\n    width = 0.3,\n    palette = {\"#56B4E9\", \"#009E73\", \"#CC79A7\"}\n)\n\n# set titles and x and y labels\np_class.set_titles(size = 18)\np_class.set_xlabels(\"Date\", fontsize = 18)\np_class.set_ylabels(\"Canopy Water Content g/cm$\\mathregular{^2}$\", fontsize = 18)\n\n<seaborn.axisgrid.FacetGrid at 0x7f22fff3f4c0>\n\n\n\n\n\n\nWhat changes in CWC do you see across vegetation types and dates?\n\n\n\n\nWe are also interested in looking at changes in LST across time, so let’s repeat a similar process to extract LST values by vegetation type. The final extracted data frame is called final_df_lst\nCreate a list of all LST files called lst_list.\n# data_dir = \"../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond\" # already loaded\n# list all files in the ewt sub-directory of the data_dir path that end in tif\nlst_list = glob.glob(os.path.join(data_dir, \"lst\", \"*.tif\"))\nCreate a subset of list of LST files using datetimes that are concurrent with EMIT aquisitions. The list is called fil_lst_list.\n\n# the date and time codes strings we want to select from our larger lst_list\nlst_dates = ['20230401T203733', '20230629T170416', '20230923T232104']\n\n# #loop so that the files appear chronologically like our lst_date list\nfil_lst_list = [] # empty list\nfor date in lst_dates:\n    # filter for appropriate dates and then add back\n    fil_lst_list.extend([file for file in lst_list if date in file])\nfil_lst_list\n\n['../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/lst/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST_dangermond.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/lst/ECOv002_L2T_LSTE_28238_012_10SGD_20230629T170416_0710_01_LST_dangermond.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/lst/ECOv002_L2T_LSTE_29576_005_10SGD_20230923T232104_0710_01_LST_dangermond.tif']\n\n\n\n\n\n# Initialize an empty list to store the plots\nplots = []\n\n# Iterate over each LST file and create a plot\nfor file in fil_lst_list:\n    # Open the raster file\n    ras = rxr.open_rasterio(file).squeeze('band', drop=True)\n    \n    \n    # Unlike the EMIT rasters there is no attribute in the raster metadata with the date. So we will\n    # extract the date string from the file name using regex.\n    ras_date =  re.search(r\"\\d{8}\", file).group() # search for groups of 8 digits\n    date_object = datetime.strptime(ras_date, \"%Y%m%d\")\n    date_only = date_object.date()\n    # Convert Kelvin to Celsius\n    ras -= 273.15  \n    \n    # Add attributes for plotting\n    long_name_value = \"Land Surface Temperature\"\n    ras.attrs['long_name'] = long_name_value\n    units_value = \"°C\"  # Change the units to Celsius\n    ras.attrs['units'] = units_value\n    \n    # Create a plot using geoviews\n    plot = ras.hvplot.image(x='x', y='y', geo=True, cmap='reds',\n                                                    tiles='ESRI', \n                                                    title=f\"{date_only} {ras.long_name} ({ras.units})\",\n                                                    xlabel='Longitude', ylabel='Latitude',\n                                                    frame_width=400, frame_height= 300,\n                                                    fontscale=1, alpha=0.7)\n    \n    # Add the plot to the list\n    plots.append(plot)\n\n# Display all plots in a grid layout using Panel\ngrid = pn.GridSpec(sizing_mode='stretch_both')\nfor i, plot in enumerate(plots):\n    grid[i // 2, i % 2] = plot  # Adjust 3 to change the number of columns\n\ngrid.servable()\n\nUnable to display output for mime type(s): \n\n\nUnable to display output for mime type(s): \n\n\n\n  \n\n\n\n\n\n\n\n# create an empty list\nex_df_lst = []\n\n# loop through each raster in fil_lst_list\nfor r in fil_lst_list:\n    # extract values \n    ras = rxr.open_rasterio(r).squeeze('band',drop=True)\n    # Convert from K to C\n    ras -= 273.15  \n    ex_ras = extract_raster_values(ras, veg_poly)\n    \n    # add date to the dataframe\n    rDate =  re.search(r\"\\d{8}\", r).group()\n    date_object = datetime.strptime(rDate, \"%Y%m%d\")\n    date_only = date_object.date()#.strftime(\"%Y-%m-%d\")\n    \n    # add column \n    ex_ras['rasDate'] = date_only\n    # add back to data frame with all values\n    ex_df_lst.append(ex_ras)\n\nfinal_df_lst = pd.concat(ex_df_lst).reset_index(drop=True)\nfinal_df_lst.rename(columns = {'value':'LST'}, inplace = True) # Rename values column to LST\nfinal_df_lst = final_df_lst[final_df_lst['LST'] > 0 ] # Removes any value less than 0\n\n# preview data\nfinal_df_lst.head()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n      spatial_ref\n      LST\n      cell_number\n      id\n      Class\n      geometry\n      rasDate\n    \n  \n  \n    \n      0\n      -120.461840\n      34.452859\n      0\n      23.470001\n      34.45285946671181_-120.46183955389516\n      0\n      ice_plant\n      MULTIPOLYGON (((-120.46214771722677 34.4533973...\n      2023-04-01\n    \n    \n      1\n      -120.461840\n      34.452317\n      0\n      23.470001\n      34.45231723419155_-120.46183955389516\n      0\n      ice_plant\n      MULTIPOLYGON (((-120.46214771722677 34.4533973...\n      2023-04-01\n    \n    \n      2\n      -120.461840\n      34.451775\n      0\n      23.570007\n      34.451775001671294_-120.46183955389516\n      0\n      ice_plant\n      MULTIPOLYGON (((-120.46214771722677 34.4533973...\n      2023-04-01\n    \n    \n      3\n      -120.461840\n      34.451233\n      0\n      23.610016\n      34.451232769151034_-120.46183955389516\n      0\n      ice_plant\n      MULTIPOLYGON (((-120.46214771722677 34.4533973...\n      2023-04-01\n    \n    \n      4\n      -120.461297\n      34.452859\n      0\n      23.470001\n      34.45285946671181_-120.4612973213749\n      0\n      ice_plant\n      MULTIPOLYGON (((-120.46214771722677 34.4533973...\n      2023-04-01\n    \n  \n\n\n\n\nLet’s take these values and make a box plot similar to before.\n\np_class_lst = sns.catplot(\n    x='rasDate', \n    y='LST', \n    col='Class',  # This will create a separate plot for each unique value in the 'Class' column\n    data=final_df_lst, \n    kind='box',\n    col_wrap=3,  # Adjust this depending on how many plots per row you want\n    sharex=False, \n    sharey=False,\n    height=5, \n    aspect=1,\n    width = 0.3,\n    palette = {\"#56B4E9\", \"#009E73\", \"#CC79A7\"}\n)\np_class_lst.set_titles(size = 18)\np_class_lst.set_xlabels(\"Date\", fontsize = 18)\np_class_lst.set_ylabels(\"Land Surface Temperature C\", fontsize = 18)\n\n<seaborn.axisgrid.FacetGrid at 0x7f22febad7e0>\n\n\n\n\n\n\nWhat changes in LST do you see across vegetation types and dates?\n\n\n\n\n\nOften times we want to visualize two variables together, so lets look at LST and CWC side by side. We have to re-write the plotting code because of seaborne formatting.\n\n# Generate subplots with boxplots for different vegetation classes comparing CWC and LST\nn_classes = ex_cwc['Class'].nunique()\nfig, axes = plt.subplots(n_classes, 2, figsize=(15, 5 * n_classes), sharex='col')\n\n# Iterate over each class and plot\nfor i, class_name in enumerate(ex_cwc['Class'].unique()):\n    # Filter data for each class\n    class_subset = ex_cwc[ex_cwc['Class'] == class_name]\n    class_lst_subset = final_df_lst[final_df_lst['Class'] == class_name]\n\n    # Create boxplot for CWC on the first column\n    sns.boxplot(\n        x='rasDate', \n        y='CWC', \n        data=class_subset, \n        ax=axes[i, 0],  # Plot on the first column\n        width=0.3, \n        palette={\"#56B4E9\", \"#009E73\", \"#CC79A7\"}\n    )\n\n    # Create boxplot for LST on the second column\n    sns.boxplot(\n        x='rasDate', \n        y='LST', \n        data=class_lst_subset, \n        ax=axes[i, 1],  # Plot on the second column\n        width=0.3, \n        palette={\"#56B4E9\", \"#009E73\", \"#CC79A7\"}\n    )\n\n    # Set titles and labels\n    axes[i, 0].set_title(f'Class {class_name} - CWC')\n    axes[i, 1].set_title(f'Class {class_name} - LST')\n    axes[i, 0].set_xlabel('Date')\n    axes[i, 0].set_ylabel('CWC')\n    axes[i, 1].set_xlabel('Date')\n    axes[i, 1].set_ylabel('LST')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nWhat do you notice about the changes in LST and CWC across vegetation types and dates? The CWC for iceplant is interesting because it remains very high across all dates indicating that these plants contain a lot of water throught the year.\n\n\n\n\n\nNow let's have  some fun with what you have learned and let you draw your own polygons to study vegetation change!\n</div>\nThere are several functions in the jldp_ras_funcs.py script that this section relies on. The interactive polygon portion is based on this notebook https://github.com/auspatious/hyperspectral-notebooks/blob/main/04_EMIT_Interactive_Polygons.ipynb with added functions to reformat the output polygons into a geopandas dataframe that we can use in the same extract_raster_values function as before.\nHere we are limiting the amount of polygons you can draw using POLY_LIMIT = 5 and are only displaying the canopy water content.\n# Limit the number of drawn polygons\nPOLY_LIMIT = 5\n\n\nHere, we will just load the canopy water content from september as our backround image. This is because we already loaded it all the way above in section 4.1 when we first visualized the polygons. We have it opened as the variable cw3, which we then copy to the ds variable for using the visualization code.\nds = cw3\nIf you wanted to change the backround image you may do so using the code below.\n# if you wanted to change the background display do it here\n# for example, let's set it to the spring image (index zero in our file list)\n# ds = rxr.open_rasterio(fil_cwc_list[0]).squeeze('band',drop=True)\nSet up plotting parameters and change the color of each polygon that we will draw.\n# create color cycle and list of colors for the number of polygons \ncolor_cycle = hv.Cycle('Category10')\ncolors = [color_cycle[i] for i in range(POLY_LIMIT)]\n\n# add map variable similar to before\nmap = ds.hvplot.image(x='x',y='y',geo=True, cmap='blues',tiles='ESRI',\n                    title=f\"{ds.long_name} ({ds.units})\",\n                 xlabel='Longitude',ylabel='Latitude', frame_width=500,\n                 frame_height=500, fontscale=1.5, alpha = 0.7)\n\n\n# Set up a holoviews points array to enable plotting of the clicked points\nxmid = ds.x.values[int(len(ds.x) / 2)]\nymid = ds.y.values[int(len(ds.y) / 2)]\n\n# create holoview polygons\npolygons = hv.Polygons(\n    [],\n    kdims=[\"Longitude\", \"latitude\"],\n)\n# stream the drawn polygons\npolygons_stream = hv.streams.PolyDraw(\n    data=polygons.columns(),\n    source=polygons,\n    num_objects=POLY_LIMIT,\n    styles={'fill_color': color_cycle.values[0:POLY_LIMIT]}\n)\nTo draw a polygon activate the Polygon Draw Tool from the toolbar on the right hand side (fourth icon from the top). Then double click somewhere on the map to start your polygon and double click to end drawing your polygon. If the hover window is in your way disable the hover tool by clicking on the last icon in the toolbar.\n\n# Plot the Map and Dynamic Map side by side\n(map * polygons)\n\nUnable to display output for mime type(s): \n\n\n\n  \n\n\n\n\nWe will use the hv_stream_to_rio_geometries function to create a geojson style list of hand-drawn polygons and then convert them to a list. This list will then be converted to a geopandas dataframe for raster extraction.\n# list out the geometries of the different polygons \nmy_geometries = hv_stream_to_rio_geometries(polygons_stream.data)\ngeo_contents = list(my_geometries)\n#contents\nNow use the create_geodataframe function from jldp_ras_funcs.py to format your drawn polygons similarly to our veg_plot_outline.geojson file. This time we will not have a class column, but will have unique id column called poly_fid column. This column will have values that represent the order in which you drew polygons above, starting with index 0.\n\n# create a geodataframe from contents\nmy_poly_gdf = create_geodataframe(geo_contents, transform_needed=True)\nmy_poly_gdf\n\n\n\n\n\n  \n    \n      \n      poly_fid\n      geometry\n    \n  \n  \n    \n      0\n      0\n      POLYGON ((-120.46423 34.45834, -120.46770 34.4...\n    \n    \n      1\n      1\n      POLYGON ((-120.37098 34.50599, -120.37869 34.5...\n    \n    \n      2\n      2\n      POLYGON ((-120.47772 34.50122, -120.48735 34.5...\n    \n  \n\n\n\n\n\n\n\nOur reference raster was variable ds.\n# extract values with previously loaded function\nex_ras = extract_raster_values(ds,my_poly_gdf)\n# remove all values less than 0 (which are -9999, all na values)\nex_ras = ex_ras[ex_ras['value'] > 0 ] \nUsing the same plotting code from above.\n\np_class_selected = sns.catplot(\n    y='value', \n    col='poly_fid',  # This will create a separate plot for each unique value in the 'poly_fid' column\n    data=ex_ras, \n    kind='box',\n    col_wrap=3,  # Adjust this depending on how many plots per row you want\n    sharex=False, \n    sharey=False,\n    height=5, \n    aspect=1,\n    width = 0.3,\n    palette = {\"#56B4E9\", \"#009E73\", \"#CC79A7\"}\n)\np_class_lst.set_titles(size = 18)\np_class_lst.set_xlabels(\"Date\", fontsize = 18)\np_class_lst.set_ylabels(\"Canopy Water Content\", fontsize = 18)\n\n<seaborn.axisgrid.FacetGrid at 0x7f22ff769a50>\n\n\n\n\n\nGreat! Our extraction and plotting worked!\n\n\n\nIf you noticed at the beginning there were more than three CWC dates available when we printed cwc_list. Let’s add two more dates to our boxplots. * 02-19-2023 (this file will say merged because it is comprised of two EMIT scenes taken on the same day) * 10-14-2023\n\n# the date and time codes strings we want to select from our larger cwc_list\n# for the first one we are using the merged 0219 dataset\ncwc_dates = ['20230219T202939_2305013_002_dangermond_cwc_merged',\n             '20230401T203751', '20230629T170449', '20230923T232101',\n             '20231014T224006']\n#loop so that the files appear chronologically like our cwc_dates list\nfil_cwc_list = [] # empty list\nfor date in cwc_dates:\n    # filter for appropriate dates and then add back\n    fil_cwc_list.extend([file for file in cwc_list if date in file])\nfil_cwc_list\n\n['../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230219T202939_2305013_002_dangermond_cwc_merged.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230401T203751_2309114_002_dangermond_cwc.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230629T170449_2318011_002_dangermond_cwc.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20230923T232101_2326615_002_dangermond_cwc.tif',\n '../../shared/2023-VITALS-Workshop-AGU/data/cropped/dangermond/ewt/EMIT_L2A_RFL_001_20231014T224006_2328715_002_dangermond_cwc.tif']\n\n\n\n\n\n\n# Initialize an empty list to store the plots\nplots = []\n\n# Iterate over each CWC file and create a plot\nfor file in fil_cwc_list:\n    # Open the raster file\n    ras = rxr.open_rasterio(file).squeeze('band', drop=True)\n    \n    # extract the date string from the file name using string matching\n    ras_date =  re.search(r\"\\d{8}\", file).group()\n    date_object = datetime.strptime(ras_date, \"%Y%m%d\")\n    date_only = date_object.date()\n    \n    # Create a plot using geoviews\n    plot = ras.hvplot.image(x='x', y='y', geo=True, cmap='blues',\n                                                    tiles='ESRI', \n                                                    title=f\"{date_only} {ras.long_name} ({ras.units})\",\n                                                    xlabel='Longitude', ylabel='Latitude',\n                                                    frame_width=400, frame_height= 300,\n                                                    fontscale=1, alpha=0.7)\n    \n    # Add the plot to the list\n    plots.append(plot)\n\n# Display all plots in a grid layout using Panel\ngrid = pn.GridSpec(sizing_mode='stretch_both')\nfor i, plot in enumerate(plots):\n    grid[i // 2, i % 2] = plot  # Adjust 3 to change the number of columns\n\ngrid.servable()\n\nUnable to display output for mime type(s): \n\n\nUnable to display output for mime type(s): \n\n\n\n\n  \n\n\n\n\n\n\n\nYou should be pretty familiar with this loop by now. We did edit the line ex_ras = extract_raster_values(ras, my_poly_gdf) to use our polygon geodataframe my_poly_gdf for raster extraction instead of veg_poly.\n# create an empty list for the extracted valeus\nex_df = []\nfor r in fil_cwc_list:\n    ## extract values\n    ras = rxr.open_rasterio(r).squeeze('band',drop=True)\n    # set raster negative values to na\n    ras.data[ras.data == -9999] = np.nan\n    ## HERE WE REPLACED VEG_POLY with my_poly_gdf\n    ex_ras = extract_raster_values(ras, my_poly_gdf)\n    \n    # add date to the dataframe\n    date_object = datetime.strptime(ras.attrs['time_coverage_end'], \"%Y-%m-%dT%H:%M:%S%z\")\n    date_only = date_object.date()\n    \n    # add column \n    ex_ras['rasDate'] = date_only\n    # add back to data frame with all values\n    ex_df.append(ex_ras)\n\nex_cwc_five = pd.concat(ex_df).reset_index(drop=True)\n\n# change the name of the extracted data column\nex_cwc_five.rename(columns = {'value':'CWC'}, inplace = True)\n# remove all nan values from the raster\nex_cwc_five = ex_cwc_five[ex_cwc_five['CWC'] != -9999 ] \nPrint boxplots for all your extracted polygons. You should have 5 dates this time.\n\nwarnings.filterwarnings('ignore')\np_class_sel = sns.catplot(\n    x='rasDate', \n    y='CWC', \n    col='poly_fid',  # This will create a separate plot for each unique value in the 'poly_fid' column\n    data=ex_cwc_five, \n    kind='box',\n    col_wrap=2,  # Adjust this depending on how many plots per row you want\n    sharex=False, \n    sharey=False,\n    height=5, \n    aspect=1,\n    width = 0.3,\n)\n\np_class_sel.set_titles(size = 18)\np_class_sel.set_xlabels(\"Date\", fontsize = 18)\np_class_sel.set_ylabels(\"Canopy Water Content g/cm$\\mathregular{^2}$\", fontsize = 18)\n\n<seaborn.axisgrid.FacetGrid at 0x7f22fede3310>\n\n\n\n\n\n\nIf you have time, see if you can repeat this process for additional LST dates. \n</div>"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "2023 AGU Workshop Schedule",
    "section": "",
    "text": "Space Station Synergies: Applying ECOSTRESS and EMIT ecological problems for Scientific Insight\n\n\n\nTime\nDescription\nLeads/Instructors\n\n\n\n\n8:00-8:30 AM\nIntroduction, overview, learning objectives\nDana Chadwick  Madeleine Pascolini-Campbell\n\n\n8:30-9:30 AM\nFinding EMIT and ECOSTRESS Data:  - Earthdata Search  - AppEEARS  - VISIONS\nAaron Friesz  Dana Chadwick\n\n\n9:30-9:45 AM\nBreak\n\n\n\n9:45-10:05 AM\nIntroduction to Cloud Computing Environment\nAaron Friesz\n\n\n10:05-10:50 AM\nFinding Concurrent EMIT and ECOSTRESS Data\nErik Bolch\n\n\n10:50-11:05 AM\nBreak\n\n\n\n11:05-12:00 PM\nExploring EMIT Reflectance and ECOSTRESS LST\nErik Bolch\n\n\n12:00-1:00 PM\nLunch Break\n\n\n\n1:00-2:00 PM\nEstimating Canopy Water Content\nDana Chadwick  Niklas Bohn  Erik Bolch\n\n\n2:00-2:30 PM\nUse Case 1: Dangermond Preserve Land Cover\nChristiana Ade  Marie Johnson\n\n\n2:30-3:00 PM\nUse Case 2: Santa Barbara Agriculture\nClaire Villanueva-Weeks  Gregory Halverson\n\n\n3:00-3:30 PM\nDiscussion, feedback, resources to take home\nAll"
  },
  {
    "objectID": "setup/prerequisites.html",
    "href": "setup/prerequisites.html",
    "title": "VITALS",
    "section": "",
    "text": "To follow along during the workshop, or to run through the notebooks contained within the repository using the Openscapes 2i2c Cloud JupyterHub (cloud workspace), the following are required. All software or accounts are free.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov/users/new\nRemember your username and password; you will need them to download or access data during the workshop and beyond.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com/join. Follow optional advice on choosing your username\nYour GitHub username is used to enable you access to a cloud environment during the workshop. To gain access, please request access to the NASA Openscapes JupyterHub using this form. You will receive an email invitation to join the organization on GitHub. You must join to gain access to the workspace.\n\n\nNetrc file\n\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\nThere are multiple methods to create a .netrc file. For this workshop, earthaccess package is used to automatically create a netrc file using your Earthdata login credentials if one does not exist. There are detailed instruction available for creating a .netrc file using other methods here.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All workshop participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2."
  },
  {
    "objectID": "setup/setup_instructions.html",
    "href": "setup/setup_instructions.html",
    "title": "VITALS",
    "section": "",
    "text": "Resources in the VITALS repository have been developed using the Openscapes 2i2c JupyterHub cloud workspace. These resources will not work as-is from a local python environment without modification. A local environment and setup instructions to run these notebooks is forthcoming.\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 12-07-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "setup/workshop_setup.html",
    "href": "setup/workshop_setup.html",
    "title": "Cloud Workspace Setup",
    "section": "",
    "text": "If you plan to use this repository with the Openscapes 2i2c JupyterHub Cloud Workspace there are no additonal setup requirements for the Python environment. All packages needed are included unless specified within a notebook, in which case a cell will be dedicated to installing the necessary Python libraries using the appropriate package manager.\nAfter completing the prerequisistes you will have access to the Openscapes 2i2c JupyterHub cloud workspace. Click here to start JupyterLab. Use your GitHub username and password to sign in. After signing in you will be prompted for some server options:\nBe sure to select the radio button for Python and a size of 14.8 GB RAM and up to 3.75 CPUs."
  },
  {
    "objectID": "setup/workshop_setup.html#troubleshooting",
    "href": "setup/workshop_setup.html#troubleshooting",
    "title": "Cloud Workspace Setup",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nWe recommend Shutting down all kernels after running each notebook. This will clear the memory used by the previous notebook, and is necessary to run some of the more memory intensive notebooks.\n\n\n\nNo single notebook exceeds roughly the limit using the provided data, but if you choose to use your own data in the notebook, or have 2 notebooks open and do not shut down the kernel, you may get an out of memory error.\nIf you elect to try this on your own data/ROI, you may need to select a larger server size. This will often happen if you are using the last EMIT scene from an orbit. In some cases those can be almost double the size of a normal scene. Please select the smallest possible."
  },
  {
    "objectID": "setup/workshop_setup.html#contact-info",
    "href": "setup/workshop_setup.html#contact-info",
    "title": "Cloud Workspace Setup",
    "section": "Contact Info",
    "text": "Contact Info\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 12-05-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "vitals.html",
    "href": "vitals.html",
    "title": "VITALS",
    "section": "",
    "text": "Please view the AGU Workshop Page for workshop details. Resources in the VITALS repository have been developed using the Openscapes 2i2c JupyterHub cloud workspace. These resources will not work as-is from a local python environment without modification.\nWelcome to the VSWIR Imaging and Thermal Applications, Learning, and Science Repository! This repository provides Python Jupyter notebooks to help users work with visibile to short-wave infrared imaging spectroscopy data, thermal infrared data, and other products from the Earth Surface Mineral Dust Source Investigation (EMIT) and ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) missions. The instruments overlapping fields of view provide an unprecedented opportunity to demonstrate the compounded benefits of working with both datasets.\nIn the interest of open science this repository has been made public but is still under active development. Make sure to consult the change log for the most recent changes to the repository. Contributions from all parties are welcome."
  },
  {
    "objectID": "vitals.html#contact-info",
    "href": "vitals.html#contact-info",
    "title": "VITALS",
    "section": "Contact Info",
    "text": "Contact Info\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 12-07-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  }
]